{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Импорт библиотек и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Библиотеки успешно импортированы!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ZubarevVV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ZubarevVV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Скачиваем стоп-слова для русского языка\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"Библиотеки успешно импортированы!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Загрузка и предварительный анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер данных: (462, 12)\n",
      "\n",
      "Первые 5 строк:\n",
      "   Hierarchy_MTR_Class  Hierarchy_MTR_Name  CSCD_ID  \\\n",
      "0     2013070303000000  Тройники фирменные  6056988   \n",
      "1     2013070303000000  Тройники фирменные  6056989   \n",
      "2     2013070303000000  Тройники фирменные  6056990   \n",
      "3     2013070303000000  Тройники фирменные  6056991   \n",
      "4     2013070303000000  Тройники фирменные  6056992   \n",
      "\n",
      "                         SHORT_NAME/ru_RU  AUTO_SHORT_NAME  \\\n",
      "0   Тройник 32х20 PPR VTp.735.0.032020032              NaN   \n",
      "1   Тройник 32х25 PPR VTp.735.0.032025032              NaN   \n",
      "2   Тройник 25х20 PPR VTp.735.0.025020025              NaN   \n",
      "3  Тройник 20х20х1/2\" PPR VTp.732.0.02004              NaN   \n",
      "4  Тройник 20х20х3/4\" PPR VTp.732.0.02005              NaN   \n",
      "\n",
      "                                     FULL_NAME/ru_RU  AUTO_FULL_NAME  \\\n",
      "0  Тройник переходной 32х20 PPR артикул VTp.735.0...             NaN   \n",
      "1  Тройник переходной 32х25 PPR артикул VTp.735.0...             NaN   \n",
      "2  Тройник переходной 25х20 PPR артикул VTp.735.0...             NaN   \n",
      "3  Тройник комбинированный 20х20х1/2\" PPR с перех...             NaN   \n",
      "4  Тройник комбинированный 20х20х3/4\" PPR с перех...             NaN   \n",
      "\n",
      "   BASE_UNIT  TMC_Stat  TMC_Type           COMMENT Item_creation_date  \n",
      "0      108.0       0.0       3.0  Эталонная запись         11.06.2025  \n",
      "1      108.0       0.0       3.0  Эталонная запись         11.06.2025  \n",
      "2      108.0       0.0       3.0  Эталонная запись         11.06.2025  \n",
      "3      108.0       0.0       3.0  Эталонная запись         11.06.2025  \n",
      "4      108.0       0.0       3.0  Эталонная запись         11.06.2025  \n",
      "\n",
      "Информация о данных:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 462 entries, 0 to 461\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Hierarchy_MTR_Class  462 non-null    int64  \n",
      " 1   Hierarchy_MTR_Name   462 non-null    object \n",
      " 2   CSCD_ID              462 non-null    int64  \n",
      " 3   SHORT_NAME/ru_RU     462 non-null    object \n",
      " 4   AUTO_SHORT_NAME      0 non-null      float64\n",
      " 5   FULL_NAME/ru_RU      462 non-null    object \n",
      " 6   AUTO_FULL_NAME       0 non-null      float64\n",
      " 7   BASE_UNIT            460 non-null    float64\n",
      " 8   TMC_Stat             460 non-null    float64\n",
      " 9   TMC_Type             460 non-null    float64\n",
      " 10  COMMENT              460 non-null    object \n",
      " 11  Item_creation_date   460 non-null    object \n",
      "dtypes: float64(5), int64(2), object(5)\n",
      "memory usage: 43.4+ KB\n",
      "None\n",
      "\n",
      "Распределение по классам:\n",
      "Hierarchy_MTR_Name\n",
      "Тройники стандартные отечественные    343\n",
      "Тройники фирменные                     46\n",
      "Тройники канализационные               40\n",
      "Тройники стандартные импортные         31\n",
      "Тройники по чертежу                     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "df = pd.read_excel('train_troyniki.xlsx')\n",
    "\n",
    "# Посмотрим на структуру данных\n",
    "print(\"Размер данных:\", df.shape)\n",
    "print(\"\\nПервые 5 строк:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nИнформация о данных:\")\n",
    "print(df.info())\n",
    "\n",
    "# Посмотрим на распределение классов\n",
    "print(\"\\nРаспределение по классам:\")\n",
    "print(df['Hierarchy_MTR_Name'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сокращение записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный датафрейм:\n",
      "Размер: (53, 12)\n",
      "Количество по категориям:\n",
      "Hierarchy_MTR_Class\n",
      "2013070101000000    29\n",
      "2013070102000000    24\n",
      "Name: count, dtype: int64\n",
      "Найдено 0 значений с количеством дубликатов > 100\n",
      "\n",
      "Результирующий датафрейм:\n",
      "Размер: (53, 12)\n",
      "Количество по категориям:\n",
      "Hierarchy_MTR_Class\n",
      "2013070101000000    29\n",
      "2013070102000000    24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def limit_duplicates(df, column_name, max_count=100):\n",
    "    \"\"\"\n",
    "    Ограничивает количество дубликатов по указанной колонке до max_count,\n",
    "    случайным образом удаляя лишние строки.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): исходный датафрейм\n",
    "    column_name (str): название колонки для анализа дубликатов\n",
    "    max_count (int): максимальное количество строк для каждого значения\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: датафрейм с ограниченным количеством дубликатов\n",
    "    \"\"\"\n",
    "    \n",
    "    # Проверяем существование колонки\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Колонка '{column_name}' не найдена в датафрейме\")\n",
    "    \n",
    "    # Создаем копию датафрейма чтобы не изменять оригинал\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Находим значения, которые встречаются больше max_count раз\n",
    "    value_counts = result_df[column_name].value_counts()\n",
    "    values_to_reduce = value_counts[value_counts > max_count].index\n",
    "    \n",
    "    print(f\"Найдено {len(values_to_reduce)} значений с количеством дубликатов > {max_count}\")\n",
    "    \n",
    "    # Для каждого значения, которое нужно сократить\n",
    "    for value in values_to_reduce:\n",
    "        # Находим индексы всех строк с этим значением\n",
    "        indices = result_df[result_df[column_name] == value].index\n",
    "        \n",
    "        # Случайным образом выбираем какие строки оставить\n",
    "        indices_to_keep = np.random.choice(indices, size=max_count, replace=False)\n",
    "        \n",
    "        # Находим индексы для удаления\n",
    "        indices_to_drop = indices.difference(indices_to_keep)\n",
    "        \n",
    "        # Удаляем лишние строки\n",
    "        result_df = result_df.drop(indices_to_drop)\n",
    "        \n",
    "        print(f\"Значение '{value}': было {len(indices)} строк, осталось {max_count}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Пример использования:\n",
    "# Создаем тестовый датафрейм\n",
    "np.random.seed(42)  # для воспроизводимости результатов\n",
    "\n",
    "\n",
    "print(\"Исходный датафрейм:\")\n",
    "print(f\"Размер: {df.shape}\")\n",
    "print(\"Количество по категориям:\")\n",
    "print(df['Hierarchy_MTR_Class'].value_counts())\n",
    "\n",
    "# Применяем функцию\n",
    "result_df = limit_duplicates(df, 'Hierarchy_MTR_Class', 100)\n",
    "\n",
    "print(\"\\nРезультирующий датафрейм:\")\n",
    "print(f\"Размер: {result_df.shape}\")\n",
    "print(\"Количество по категориям:\")\n",
    "print(result_df['Hierarchy_MTR_Class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hierarchy_MTR_Class</th>\n",
       "      <th>Hierarchy_MTR_Name</th>\n",
       "      <th>CSCD_ID</th>\n",
       "      <th>SHORT_NAME/ru_RU</th>\n",
       "      <th>AUTO_SHORT_NAME</th>\n",
       "      <th>FULL_NAME/ru_RU</th>\n",
       "      <th>AUTO_FULL_NAME</th>\n",
       "      <th>BASE_UNIT</th>\n",
       "      <th>TMC_Stat</th>\n",
       "      <th>TMC_Type</th>\n",
       "      <th>COMMENT</th>\n",
       "      <th>Item_creation_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.29.23.120</td>\n",
       "      <td>Предметы домашнего обихода пластмассовые прочие</td>\n",
       "      <td>2331818</td>\n",
       "      <td>Органайзер д/раковины, диспенсер д/мыла</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Органайзер для раковины, диспенсер для мыла</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.07.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.02.10.120</td>\n",
       "      <td>Шкафы кухонные</td>\n",
       "      <td>2269245</td>\n",
       "      <td>Шкаф для посуды навесной 800х300х700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Шкаф для посуды навесной 2-х створчатый 800х30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.04.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.02.10.120</td>\n",
       "      <td>Шкафы кухонные</td>\n",
       "      <td>2269247</td>\n",
       "      <td>Шкаф для посуды напольный 800х600х750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Шкаф для посуды напольный 2-х створчатый 800х6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.04.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.13.13.112</td>\n",
       "      <td>Посуда столовая и кухонная из прочего стекла</td>\n",
       "      <td>2129707</td>\n",
       "      <td>Точилка Regent inox Promo для ножей</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Точилка Regent inox Linea Promo для ножей 14х6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.07.2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.20.11.129</td>\n",
       "      <td>Обувь из полимерных материалов прочая, не вклю...</td>\n",
       "      <td>2282666</td>\n",
       "      <td>Сабо Lucky Land 4225 W-S-EVA-2C жен.р.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Сабо Lucky Land 4225 W-S-EVA-2C повседневные ж...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.05.2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hierarchy_MTR_Class                                 Hierarchy_MTR_Name  \\\n",
       "0        22.29.23.120    Предметы домашнего обихода пластмассовые прочие   \n",
       "3        31.02.10.120                                     Шкафы кухонные   \n",
       "4        31.02.10.120                                     Шкафы кухонные   \n",
       "6        23.13.13.112       Посуда столовая и кухонная из прочего стекла   \n",
       "7        15.20.11.129  Обувь из полимерных материалов прочая, не вклю...   \n",
       "\n",
       "   CSCD_ID                          SHORT_NAME/ru_RU  AUTO_SHORT_NAME  \\\n",
       "0  2331818   Органайзер д/раковины, диспенсер д/мыла              NaN   \n",
       "3  2269245      Шкаф для посуды навесной 800х300х700              NaN   \n",
       "4  2269247     Шкаф для посуды напольный 800х600х750              NaN   \n",
       "6  2129707       Точилка Regent inox Promo для ножей              NaN   \n",
       "7  2282666  Сабо Lucky Land 4225 W-S-EVA-2C жен.р.36              NaN   \n",
       "\n",
       "                                     FULL_NAME/ru_RU  AUTO_FULL_NAME  \\\n",
       "0        Органайзер для раковины, диспенсер для мыла             NaN   \n",
       "3  Шкаф для посуды навесной 2-х створчатый 800х30...             NaN   \n",
       "4  Шкаф для посуды напольный 2-х створчатый 800х6...             NaN   \n",
       "6  Точилка Regent inox Linea Promo для ножей 14х6...             NaN   \n",
       "7  Сабо Lucky Land 4225 W-S-EVA-2C повседневные ж...             NaN   \n",
       "\n",
       "   BASE_UNIT  TMC_Stat  TMC_Type COMMENT Item_creation_date  \n",
       "0        108         0         4     NaN         24.07.2025  \n",
       "3        108         0         4     NaN         17.04.2025  \n",
       "4        108         0         4     NaN         17.04.2025  \n",
       "6        108         0         3     NaN         11.07.2024  \n",
       "7        108         0         3     NaN         15.05.2025  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = result_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Предобработка текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры обработанных текстов:\n",
      "Оригинал: Тройник 32х20 PPR VTp.735.0.032020032\n",
      "Обработанный: тройник 32 x 20 ppr vtp 735 0 032020032 тройник пере x одной 32 x 20 ppr артикул vtp 735 0 032020032 valtec\n",
      "--------------------------------------------------\n",
      "Оригинал: Тройник 32х25 PPR VTp.735.0.032025032\n",
      "Обработанный: тройник 32 x 25 ppr vtp 735 0 032025032 тройник пере x одной 32 x 25 ppr артикул vtp 735 0 032025032 valtec\n",
      "--------------------------------------------------\n",
      "Оригинал: Тройник 25х20 PPR VTp.735.0.025020025\n",
      "Обработанный: тройник 25 x 20 ppr vtp 735 0 025020025 тройник пере x одной 25 x 20 ppr артикул vtp 735 0 025020025 valtec\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# === ЗАМЕНИТЕ существующую функцию preprocess_text на эту ===\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаляем специальные символы, но сохраняем числа и размеры\n",
    "    text = re.sub(r'[^\\w\\s\\dх×]', ' ', text)\n",
    "    \n",
    "    # Заменяем различные варианты написания 'х' на стандартный\n",
    "    text = re.sub(r'[х×]', ' x ', text)\n",
    "    \n",
    "    # Удаляем лишние пробелы\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# === ДОБАВЬТЕ НОВУЮ ФУНКЦИЮ АУГМЕНТАЦИИ ===\n",
    "def augment_technical_text(text, augmentation_type=\"synonym\"):\n",
    "    \"\"\"\n",
    "    Аугментация технических текстов с сохранением смысла\n",
    "    \"\"\"\n",
    "    if len(text.split()) < 3:  # Слишком короткие тексты не аугментируем\n",
    "        return text\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    if augmentation_type == \"synonym\":\n",
    "        # Замена синонимами технических терминов\n",
    "        synonyms = {\n",
    "            'фильтр': 'фильтроэлемент',\n",
    "            'клапан': 'вентиль', \n",
    "            'насос': 'помпа',\n",
    "            'болт': 'крепеж',\n",
    "            'гайка': 'крепежный элемент',\n",
    "            'труба': 'трубопровод',\n",
    "            'масло': 'смазка',\n",
    "            'подшипник': 'подшипниковый узел',\n",
    "            'муфта': 'соединитель',\n",
    "            'шланг': 'рукав'\n",
    "        }\n",
    "        \n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word in synonyms and np.random.random() > 0.7:\n",
    "                new_words.append(synonyms[word])\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    elif augmentation_type == \"permutation\":\n",
    "        # Перестановка слов в пределах смысловых блоков\n",
    "        if len(words) > 4:\n",
    "            # Находим прилагательные и существительные\n",
    "            nouns_adj = [i for i, word in enumerate(words) if len(word) > 3]\n",
    "            if len(nouns_adj) >= 2:\n",
    "                # Меняем местами два случайных слова\n",
    "                idx1, idx2 = np.random.choice(nouns_adj, 2, replace=False)\n",
    "                words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    elif augmentation_type == \"insert_noise\":\n",
    "        # Вставка технических стоп-слов\n",
    "        technical_words = [\"тип\", \"марка\", \"модель\", \"размер\", \"стандарт\"]\n",
    "        if np.random.random() > 0.8 and len(words) > 2:\n",
    "            insert_pos = np.random.randint(1, len(words))\n",
    "            words.insert(insert_pos, np.random.choice(technical_words))\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "# Применяем предобработку к текстовым полям\n",
    "df['processed_short_name'] = df['SHORT_NAME/ru_RU'].apply(preprocess_text)\n",
    "df['processed_full_name'] = df['FULL_NAME/ru_RU'].apply(preprocess_text)\n",
    "\n",
    "# Объединяем короткое и полное название для лучшего контекста\n",
    "df['combined_text'] = df['processed_short_name'] + ' ' + df['processed_full_name']\n",
    "\n",
    "# Посмотрим на результат предобработки\n",
    "print(\"Примеры обработанных текстов:\")\n",
    "for i in range(3):\n",
    "    print(f\"Оригинал: {df['SHORT_NAME/ru_RU'].iloc[i]}\")\n",
    "    print(f\"Обработанный: {df['combined_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Подготовка данных для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_three_way(X, y, labels, val_size=0.15, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Стратифицированное разделение на train/val/test с учетом размера классов\n",
    "    X - матрица признаков\n",
    "    y - one-hot encoded метки  \n",
    "    labels - оригинальные метки (не one-hot)\n",
    "    val_size - доля валидационной выборки от общего объема\n",
    "    test_size - доля тестовой выборки от общего объема\n",
    "    \"\"\"\n",
    "    df_temp = pd.DataFrame({'text_idx': range(len(X)), 'label': labels})\n",
    "    class_sizes = df_temp['label'].value_counts()\n",
    "    \n",
    "    # Разделяем классы на группы по размеру\n",
    "    large_classes = class_sizes[class_sizes > 100].index\n",
    "    medium_classes = class_sizes[(class_sizes >= 20) & (class_sizes <= 100)].index\n",
    "    small_classes = class_sizes[class_sizes < 20].index\n",
    "    \n",
    "    print(\"=== РАСПРЕДЕЛЕНИЕ КЛАССОВ ПО ГРУППАМ ===\")\n",
    "    print(f\"Большие классы (>100 примеров): {len(large_classes)}\")\n",
    "    print(f\"Средние классы (20-100 примеров): {len(medium_classes)}\")\n",
    "    print(f\"Малые классы (<20 примеров): {len(small_classes)}\")\n",
    "    \n",
    "    train_indices, val_indices, test_indices = [], [], []\n",
    "    \n",
    "    # Обрабатываем каждую группу отдельно\n",
    "    for i, class_group in enumerate([large_classes, medium_classes, small_classes]):\n",
    "        if len(class_group) > 0:\n",
    "            group_indices = df_temp[df_temp['label'].isin(class_group)]['text_idx'].values\n",
    "            \n",
    "            group_name = [\"Большие\", \"Средние\", \"Малые\"][i]\n",
    "            print(f\"\\n{group_name} классы: {len(group_indices)} примеров\")\n",
    "            \n",
    "            if len(group_indices) > 2:  # Нужно минимум 3 примера для train/val/test\n",
    "                # Сначала разделяем на train и temp\n",
    "                train_idx, temp_idx = train_test_split(\n",
    "                    group_indices, \n",
    "                    test_size=(val_size + test_size), \n",
    "                    random_state=42,\n",
    "                    stratify=df_temp.iloc[group_indices]['label'].values\n",
    "                )\n",
    "                \n",
    "                # Затем temp разделяем на val и test\n",
    "                val_idx, test_idx = train_test_split(\n",
    "                    temp_idx,\n",
    "                    test_size=test_size/(val_size + test_size),  # Корректируем пропорцию\n",
    "                    random_state=42,\n",
    "                    stratify=df_temp.iloc[temp_idx]['label'].values\n",
    "                )\n",
    "                \n",
    "                train_indices.extend(train_idx)\n",
    "                val_indices.extend(val_idx)\n",
    "                test_indices.extend(test_idx)\n",
    "                print(f\"  → Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "                \n",
    "            elif len(group_indices) == 2:\n",
    "                # Для 2 примеров: 1 в train, 1 в val\n",
    "                train_idx, val_idx = train_test_split(\n",
    "                    group_indices, \n",
    "                    test_size=0.5, \n",
    "                    random_state=42,\n",
    "                    stratify=df_temp.iloc[group_indices]['label'].values\n",
    "                )\n",
    "                train_indices.extend(train_idx)\n",
    "                val_indices.extend(val_idx)\n",
    "                test_indices.extend([])  # Пустой test\n",
    "                print(f\"  → Train: {len(train_idx)}, Val: {len(val_idx)}, Test: 0\")\n",
    "                \n",
    "            else:\n",
    "                # Для 1 примера - только в train\n",
    "                train_indices.extend(group_indices)\n",
    "                print(f\"  → Train: {len(group_indices)}, Val: 0, Test: 0\")\n",
    "    \n",
    "    # Преобразуем индексы обратно в массивы\n",
    "    X_train, X_val, X_test = X[train_indices], X[val_indices], X[test_indices]\n",
    "    y_train, y_val, y_test = y[train_indices], y[val_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соответствие меток:\n",
      "\n",
      "=== АНАЛИЗ РАСПРЕДЕЛЕНИЯ КЛАССОВ ===\n",
      "Большие классы (>100 примеров): 1\n",
      "Средние классы (20-100 примеров): 3\n",
      "Малые классы (2-19 примеров): 1\n",
      "Единичные классы (1 пример): 0\n",
      "Всего классов: 5\n",
      "0: Тройники канализационные\n",
      "1: Тройники по чертежу\n",
      "2: Тройники стандартные импортные\n",
      "3: Тройники стандартные отечественные\n",
      "4: Тройники фирменные\n",
      "Размерность данных: (462, 100)\n",
      "Количество классов: 5\n",
      "Размер словаря: 347\n"
     ]
    }
   ],
   "source": [
    "# Кодируем целевые метки\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_labels'] = label_encoder.fit_transform(df['Hierarchy_MTR_Name'])\n",
    "\n",
    "# Сохраняем mapping для обратного преобразования\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Соответствие меток:\")\n",
    "\n",
    "print(\"\\n=== АНАЛИЗ РАСПРЕДЕЛЕНИЯ КЛАССОВ ===\")\n",
    "class_counts = df['Hierarchy_MTR_Name'].value_counts()\n",
    "\n",
    "large_classes = class_counts[class_counts > 100]\n",
    "medium_classes = class_counts[(class_counts >= 20) & (class_counts <= 100)]\n",
    "small_classes = class_counts[(class_counts >= 2) & (class_counts < 20)]\n",
    "single_classes = class_counts[class_counts == 1]\n",
    "\n",
    "print(f\"Большие классы (>100 примеров): {len(large_classes)}\")\n",
    "print(f\"Средние классы (20-100 примеров): {len(medium_classes)}\") \n",
    "print(f\"Малые классы (2-19 примеров): {len(small_classes)}\")\n",
    "print(f\"Единичные классы (1 пример): {len(single_classes)}\")\n",
    "print(f\"Всего классов: {len(class_counts)}\")\n",
    "\n",
    "# Сохраняем информацию о классах для балансировки\n",
    "class_info = {\n",
    "    'large': large_classes.index.tolist(),\n",
    "    'medium': medium_classes.index.tolist(), \n",
    "    'small': small_classes.index.tolist(),\n",
    "    'single': single_classes.index.tolist()\n",
    "}\n",
    "\n",
    "for label, encoded in label_mapping.items():\n",
    "    print(f\"{encoded}: {label}\")\n",
    "\n",
    "# Подготовка текстовых данных\n",
    "texts = df['combined_text'].tolist()\n",
    "labels = df['encoded_labels'].values\n",
    "\n",
    "# Параметры токенизации\n",
    "MAX_NB_WORDS = 10000  # Максимальное количество слов в словаре\n",
    "MAX_SEQUENCE_LENGTH = 100  # Максимальная длина последовательности\n",
    "EMBEDDING_DIM = 100  # Размерность векторного представления\n",
    "\n",
    "# Токенизация текстов\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Паддинг последовательностей до одинаковой длины\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Подготовка меток\n",
    "y = to_categorical(labels)\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "print(f\"Размерность данных: {X.shape}\")\n",
    "print(f\"Количество классов: {num_classes}\")\n",
    "print(f\"Размер словаря: {len(tokenizer.word_index)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Анализ распределения классов и решение проблемы малых классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение классов:\n",
      "Hierarchy_MTR_Name\n",
      "Тройники стандартные отечественные    343\n",
      "Тройники фирменные                     46\n",
      "Тройники канализационные               40\n",
      "Тройники стандартные импортные         31\n",
      "Тройники по чертежу                     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Классы с достаточным количеством данных (5):\n",
      "['Тройники стандартные отечественные', 'Тройники фирменные', 'Тройники канализационные', 'Тройники стандартные импортные', 'Тройники по чертежу']\n",
      "\n",
      "Мелкие классы (будут исключены из обучения) (0):\n",
      "[]\n",
      "\n",
      "Исходный размер датасета: 462\n",
      "Размер после фильтрации: 462\n",
      "Удалено записей: 0\n",
      "\n",
      "Соответствие меток после фильтрации:\n",
      "0: Тройники канализационные (примеров: 40)\n",
      "1: Тройники по чертежу (примеров: 2)\n",
      "2: Тройники стандартные импортные (примеров: 31)\n",
      "3: Тройники стандартные отечественные (примеров: 343)\n",
      "4: Тройники фирменные (примеров: 46)\n"
     ]
    }
   ],
   "source": [
    "# Анализируем распределение классов\n",
    "class_distribution = df['Hierarchy_MTR_Name'].value_counts()\n",
    "print(\"Распределение классов:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Определяем порог для малых классов\n",
    "MIN_SAMPLES_PER_CLASS = 0  # Минимальное количество примеров для стратификации\n",
    "\n",
    "# Находим классы с достаточным количеством примеров\n",
    "valid_classes = class_distribution[class_distribution >= MIN_SAMPLES_PER_CLASS].index\n",
    "small_classes = class_distribution[class_distribution < MIN_SAMPLES_PER_CLASS].index\n",
    "\n",
    "print(f\"\\nКлассы с достаточным количеством данных ({len(valid_classes)}):\")\n",
    "print(valid_classes.tolist())\n",
    "\n",
    "print(f\"\\nМелкие классы (будут исключены из обучения) ({len(small_classes)}):\")\n",
    "print(small_classes.tolist())\n",
    "\n",
    "# Фильтруем данные, оставляя только классы с достаточным количеством примеров\n",
    "df_filtered = df[df['Hierarchy_MTR_Name'].isin(valid_classes)].copy()\n",
    "\n",
    "print(f\"\\nИсходный размер датасета: {len(df)}\")\n",
    "print(f\"Размер после фильтрации: {len(df_filtered)}\")\n",
    "print(f\"Удалено записей: {len(df) - len(df_filtered)}\")\n",
    "\n",
    "# Перекодируем метки для отфильтрованных данных\n",
    "label_encoder = LabelEncoder()\n",
    "df_filtered['encoded_labels'] = label_encoder.fit_transform(df_filtered['Hierarchy_MTR_Name'])\n",
    "\n",
    "# Сохраняем mapping для обратного преобразования\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"\\nСоответствие меток после фильтрации:\")\n",
    "for label, encoded in label_mapping.items():\n",
    "    print(f\"{encoded}: {label} (примеров: {class_distribution[label]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Добавляем функцию балансировки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Применяем балансировку датасета...\n",
      "\n",
      "=== НАЧИНАЕМ БАЛАНСИРОВКУ ДАННЫХ ===\n",
      "\n",
      "Обрабатываем large классы...\n",
      "\n",
      "Обрабатываем medium классы...\n",
      "\n",
      "Обрабатываем small классы...\n",
      "  Тройники по чертежу: 2 → 20 (+18)\n",
      "\n",
      "Обрабатываем single классы...\n",
      "\n",
      "=== РЕЗУЛЬТАТЫ БАЛАНСИРОВКИ ===\n",
      "Исходный размер: 462\n",
      "После балансировки: 480\n",
      "Добавлено примеров: 18\n"
     ]
    }
   ],
   "source": [
    "# === ДОБАВЬТЕ ФУНКЦИЮ БАЛАНСИРОВКИ ===\n",
    "def balance_dataset(df, class_info):\n",
    "    \"\"\"\n",
    "    Балансировка датасета через аугментацию малых классов\n",
    "    \"\"\"\n",
    "    balanced_dfs = []\n",
    "    \n",
    "    # Целевое количество примеров для каждого типа классов\n",
    "    target_counts = {\n",
    "        'large': None,      # Не изменяем\n",
    "        'medium': None,      # Доводим до 100\n",
    "        'small': 20,       # Доводим до 150  \n",
    "        'single': 10       # Доводим до 200\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== НАЧИНАЕМ БАЛАНСИРОВКУ ДАННЫХ ===\")\n",
    "    \n",
    "    # Обрабатываем каждый тип классов\n",
    "    for class_type, classes in class_info.items():\n",
    "        print(f\"\\nОбрабатываем {class_type} классы...\")\n",
    "        \n",
    "        for class_name in classes:\n",
    "            class_df = df[df['Hierarchy_MTR_Name'] == class_name]\n",
    "            current_count = len(class_df)\n",
    "            \n",
    "            if class_type == 'large' or class_type == 'medium':\n",
    "                # Большие классы не изменяем\n",
    "                balanced_dfs.append(class_df)\n",
    "                continue\n",
    "                \n",
    "            target_count = target_counts[class_type]\n",
    "            needed = target_count - current_count\n",
    "            \n",
    "            if needed > 0:\n",
    "                print(f\"  {class_name}: {current_count} → {target_count} (+{needed})\")\n",
    "                \n",
    "                # Создаем аугментированные примеры\n",
    "                augmented_texts = []\n",
    "                augmentation_types = [\"synonym\", \"permutation\", \"insert_noise\"]\n",
    "                \n",
    "                for i in range(needed):\n",
    "                    # Берем случайный оригинальный пример\n",
    "                    original_text = class_df.sample(1)['combined_text'].iloc[0]\n",
    "                    # Выбираем случайный тип аугментации\n",
    "                    aug_type = np.random.choice(augmentation_types)\n",
    "                    # Создаем аугментированный текст\n",
    "                    augmented_text = augment_technical_text(original_text, aug_type)\n",
    "                    augmented_texts.append(augmented_text)\n",
    "                \n",
    "                # Создаем DataFrame с аугментированными данными\n",
    "                augmented_df = pd.DataFrame({\n",
    "                    'Hierarchy_MTR_Name': [class_name] * len(augmented_texts),\n",
    "                    'combined_text': augmented_texts,\n",
    "                    'SHORT_NAME/ru_RU': ['AUGMENTED'] * len(augmented_texts),\n",
    "                    'FULL_NAME/ru_RU': ['AUGMENTED'] * len(augmented_texts),\n",
    "                    'is_augmented': [True] * len(augmented_texts)\n",
    "                })\n",
    "                \n",
    "                # Объединяем оригинальные и аугментированные данные\n",
    "                balanced_dfs.append(pd.concat([class_df, augmented_df], ignore_index=True))\n",
    "            else:\n",
    "                balanced_dfs.append(class_df)\n",
    "    \n",
    "    # Объединяем все обратно\n",
    "    result_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n=== РЕЗУЛЬТАТЫ БАЛАНСИРОВКИ ===\")\n",
    "    print(f\"Исходный размер: {len(df)}\")\n",
    "    print(f\"После балансировки: {len(result_df)}\")\n",
    "    print(f\"Добавлено примеров: {len(result_df) - len(df)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Применяем балансировку\n",
    "print(\"\\nПрименяем балансировку датасета...\")\n",
    "df_balanced = balance_dataset(df, class_info)\n",
    "\n",
    "# Обновляем тексты и метки после балансировки\n",
    "texts = df_balanced['combined_text'].tolist()\n",
    "labels = label_encoder.transform(df_balanced['Hierarchy_MTR_Name'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Подготовка текстовых данных для отфильтрованного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ФИНАЛЬНАЯ СТАТИСТИКА ===\n",
      "Размерность данных: (480, 100)\n",
      "Количество классов: 5\n",
      "Размер словаря: 347\n",
      "\n",
      "Распределение после балансировки:\n",
      "Минимальное количество примеров в классе: 20\n",
      "Максимальное количество примеров в классе: 343\n",
      "Среднее количество примеров в классе: 96.0\n"
     ]
    }
   ],
   "source": [
    "# === ЗАМЕНИТЕ существующий код токенизации на этот ===\n",
    "\n",
    "# Токенизация текстов с обновленным словарем\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Паддинг последовательностей до одинаковой длины\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Подготовка меток\n",
    "y = to_categorical(labels)\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "print(f\"\\n=== ФИНАЛЬНАЯ СТАТИСТИКА ===\")\n",
    "print(f\"Размерность данных: {X.shape}\")\n",
    "print(f\"Количество классов: {num_classes}\")\n",
    "print(f\"Размер словаря: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# Анализ распределения после балансировки\n",
    "balanced_class_counts = df_balanced['Hierarchy_MTR_Name'].value_counts()\n",
    "print(f\"\\nРаспределение после балансировки:\")\n",
    "print(f\"Минимальное количество примеров в классе: {balanced_class_counts.min()}\")\n",
    "print(f\"Максимальное количество примеров в классе: {balanced_class_counts.max()}\")\n",
    "print(f\"Среднее количество примеров в классе: {balanced_class_counts.mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Альтернативное решение: увеличение веса малых классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ВЕСА КЛАССОВ ===\n",
      "Минимальный вес: 0.28\n",
      "Максимальный вес: 4.80\n",
      "Медианный вес: 2.40\n"
     ]
    }
   ],
   "source": [
    "# === ДОБАВЬТЕ УЛУЧШЕННОЕ ВЗВЕШИВАНИЕ КЛАССОВ ===\n",
    "\n",
    "def compute_balanced_class_weights(y):\n",
    "    \"\"\"\n",
    "    Вычисление сбалансированных весов классов с учетом аугментации\n",
    "    \"\"\"\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    # Получаем числовые метки\n",
    "    y_labels = np.argmax(y, axis=1)\n",
    "    \n",
    "    # Вычисляем веса\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_labels),\n",
    "        y=y_labels\n",
    "    )\n",
    "    \n",
    "    # Создаем словарь весов\n",
    "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    \n",
    "    print(\"\\n=== ВЕСА КЛАССОВ ===\")\n",
    "    print(f\"Минимальный вес: {min(class_weights):.2f}\")\n",
    "    print(f\"Максимальный вес: {max(class_weights):.2f}\")\n",
    "    print(f\"Медианный вес: {np.median(class_weights):.2f}\")\n",
    "    \n",
    "    return class_weight_dict\n",
    "\n",
    "# Вычисляем веса классов\n",
    "class_weight_dict = compute_balanced_class_weights(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПРОСТАЯ АРХИТЕКТУРА МОДЕЛИ ===\n",
      "Параметры модели: 58,549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZubarevVV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ZubarevVV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:970: UserWarning: Layer 'conv1d_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_3          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │        \u001b[38;5;34m34,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m19,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_3          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,549</span> (228.71 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,549\u001b[0m (228.71 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,549</span> (228.71 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,549\u001b[0m (228.71 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_simple_model(vocab_size, embedding_dim, sequence_length, num_classes):\n",
    "    \"\"\"\n",
    "    Простая архитектура, которая уже доказала эффективность\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        Conv1D(64, 3, activation='relu'),  # Всего 1 слой свертки\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation='relu'),      # Уменьшили с 128 до 64\n",
    "        Dropout(0.3),                      # Уменьшили dropout\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Явно строим модель\n",
    "    model.build(input_shape=(None, sequence_length))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Создаем упрощенную модель\n",
    "model = create_simple_model(\n",
    "    vocab_size=min(MAX_NB_WORDS, len(tokenizer.word_index) + 1),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "print(\"=== ПРОСТАЯ АРХИТЕКТУРА МОДЕЛИ ===\")\n",
    "print(f\"Параметры модели: {model.count_params():,}\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Обучение модели с учетом дисбаланса (измененный блок обучения)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Разделение на train/validation без СТРАТИФИКАЦИИ\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=np.argmax(y, axis=1)\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=np.argmax(y_temp, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\n=== РАЗБИЕНИЕ ДАННЫХ ===\")\n",
    "print(f\"Обучающая выборка: {X_train.shape[0]}\")\n",
    "print(f\"Валидационная выборка: {X_val.shape[0]}\") \n",
    "print(f\"Тестовая выборка: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ПРИМЕНЕНИЕ СТРАТИФИЦИРОВАННОГО РАЗДЕЛЕНИЯ НА TRAIN/VAL/TEST\n",
      "============================================================\n",
      "=== РАСПРЕДЕЛЕНИЕ КЛАССОВ ПО ГРУППАМ ===\n",
      "Большие классы (>100 примеров): 1\n",
      "Средние классы (20-100 примеров): 4\n",
      "Малые классы (<20 примеров): 0\n",
      "\n",
      "Большие классы: 343 примеров\n",
      "  → Train: 240, Val: 51, Test: 52\n",
      "\n",
      "Средние классы: 137 примеров\n",
      "  → Train: 95, Val: 21, Test: 21\n",
      "\n",
      "========================================\n",
      "ИТОГОВЫЕ РАЗМЕРЫ НАБОРОВ ДАННЫХ\n",
      "========================================\n",
      "X_train: (335, 100), y_train: (335, 5)\n",
      "X_val: (72, 100), y_val: (72, 5)\n",
      "X_test: (73, 100), y_test: (73, 5)\n",
      "\n",
      "РАСПРЕДЕЛЕНИЕ КЛАССОВ:\n",
      "Уникальных классов в train: 5\n",
      "Уникальных классов в val: 5\n",
      "Уникальных классов в test: 5\n",
      "\n",
      "Общее количество примеров: 480\n",
      "Пропорции: Train 69.8%, Val 15.0%, Test 15.2%\n"
     ]
    }
   ],
   "source": [
    "# === ЗАМЕНЯЕМ ВАШЕ ТЕКУЩЕЕ РАЗДЕЛЕНИЕ НА ЭТО ===\n",
    "# Разделение на train/validation СО СТРАТИФИКАЦИЕЙ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ПРИМЕНЕНИЕ СТРАТИФИЦИРОВАННОГО РАЗДЕЛЕНИЯ НА TRAIN/VAL/TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Сохраняем оригинальные метки для стратификации\n",
    "original_labels = labels  # это массив с числовыми метками (не one-hot)\n",
    "\n",
    "# Применяем наше улучшенное разделение\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_split_three_way(\n",
    "    X, y, original_labels, val_size=0.15, test_size=0.15\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*40)\n",
    "print(\"ИТОГОВЫЕ РАЗМЕРЫ НАБОРОВ ДАННЫХ\")\n",
    "print(\"=\"*40)\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\") \n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Проверяем распределение классов\n",
    "unique_train = len(np.unique(np.argmax(y_train, axis=1)))\n",
    "unique_val = len(np.unique(np.argmax(y_val, axis=1)))\n",
    "unique_test = len(np.unique(np.argmax(y_test, axis=1)))\n",
    "\n",
    "print(f\"\\nРАСПРЕДЕЛЕНИЕ КЛАССОВ:\")\n",
    "print(f\"Уникальных классов в train: {unique_train}\")\n",
    "print(f\"Уникальных классов в val: {unique_val}\")\n",
    "print(f\"Уникальных классов в test: {unique_test}\")\n",
    "\n",
    "# Проверяем общее количество примеров\n",
    "total_samples = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"\\nОбщее количество примеров: {total_samples}\")\n",
    "print(f\"Пропорции: Train {len(X_train)/total_samples:.1%}, Val {len(X_val)/total_samples:.1%}, Test {len(X_test)/total_samples:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальный индекс в данных: 345\n",
      "Размер словаря: 348\n",
      "Сумма по строкам y_train: min=1.00, max=1.00\n"
     ]
    }
   ],
   "source": [
    "# Дополнительная проверка данных\n",
    "def validate_data(X_train, y_train, X_val, y_val, tokenizer):\n",
    "    \"\"\"Проверка целостности данных перед обучением\"\"\"\n",
    "    \n",
    "    # Проверяем, что все индексы в пределах словаря\n",
    "    max_index = np.max(X_train)\n",
    "    vocab_size_actual = len(tokenizer.word_index) + 1  # +1 для padding\n",
    "    print(f\"Максимальный индекс в данных: {max_index}\")\n",
    "    print(f\"Размер словаря: {vocab_size_actual}\")\n",
    "    \n",
    "    if max_index >= vocab_size_actual:\n",
    "        print(f\"ОШИБКА: Найден индекс {max_index}, но размер словаря только {vocab_size_actual}\")\n",
    "        # Исправляем проблему\n",
    "        X_train = np.clip(X_train, 0, vocab_size_actual - 1)\n",
    "        X_val = np.clip(X_val, 0, vocab_size_actual - 1)\n",
    "        print(\"Данные исправлены через clipping\")\n",
    "    \n",
    "    # Проверяем соответствие размерностей\n",
    "    assert X_train.shape[0] == y_train.shape[0], \"Несоответствие размеров X_train и y_train\"\n",
    "    assert X_val.shape[0] == y_val.shape[0], \"Несоответствие размеров X_val и y_val\"\n",
    "    \n",
    "    # Проверяем, что y содержит вероятности\n",
    "    y_sum = np.sum(y_train, axis=1)\n",
    "    print(f\"Сумма по строкам y_train: min={np.min(y_sum):.2f}, max={np.max(y_sum):.2f}\")\n",
    "    \n",
    "    return X_train, X_val\n",
    "\n",
    "# Проверяем и исправляем данные если нужно\n",
    "X_train, X_val = validate_data(X_train, y_train, X_val, y_val, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== НАЧИНАЕМ ОБУЧЕНИЕ ===\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZubarevVV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:970: UserWarning: Layer 'conv1d_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.3672 - loss: 1.5525 - val_accuracy: 0.8472 - val_loss: 1.4732\n",
      "Epoch 2/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8448 - loss: 1.3882 - val_accuracy: 0.9167 - val_loss: 1.1693\n",
      "Epoch 3/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9582 - loss: 1.1434 - val_accuracy: 1.0000 - val_loss: 0.7520\n",
      "Epoch 4/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9761 - loss: 0.8121 - val_accuracy: 1.0000 - val_loss: 0.4132\n",
      "Epoch 5/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9910 - loss: 0.5018 - val_accuracy: 1.0000 - val_loss: 0.2400\n",
      "Epoch 6/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9821 - loss: 0.2683 - val_accuracy: 1.0000 - val_loss: 0.1328\n",
      "Epoch 7/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9940 - loss: 0.1191 - val_accuracy: 1.0000 - val_loss: 0.0708\n",
      "Epoch 8/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0758 - val_accuracy: 1.0000 - val_loss: 0.0406\n",
      "Epoch 9/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0430 - val_accuracy: 1.0000 - val_loss: 0.0258\n",
      "Epoch 10/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9970 - loss: 0.0280 - val_accuracy: 1.0000 - val_loss: 0.0175\n",
      "Epoch 11/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9970 - loss: 0.0307 - val_accuracy: 1.0000 - val_loss: 0.0129\n",
      "Epoch 12/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9970 - loss: 0.0151 - val_accuracy: 1.0000 - val_loss: 0.0095\n",
      "Epoch 13/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0079\n",
      "Epoch 14/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
      "Epoch 15/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0164 - val_accuracy: 1.0000 - val_loss: 0.0060\n",
      "Epoch 16/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0055\n",
      "Epoch 17/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 0.0049\n",
      "Epoch 18/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0043\n",
      "Epoch 19/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
      "Epoch 20/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
      "Epoch 21/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
      "Epoch 22/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 0.0031\n",
      "Epoch 23/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 24/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0026\n",
      "Epoch 25/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9970 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
      "Epoch 26/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
      "Epoch 27/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 28/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "Epoch 29/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 30/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 31/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 32/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 33/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 34/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 35/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 36/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 9.9131e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 9.6611e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 9.2375e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 8.9218e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.9218e-04 - val_accuracy: 1.0000 - val_loss: 8.8861e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 8.6959e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.9623e-04 - val_accuracy: 1.0000 - val_loss: 8.6824e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.8664e-04 - val_accuracy: 1.0000 - val_loss: 8.6907e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.6863e-04 - val_accuracy: 1.0000 - val_loss: 8.8983e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 8.7282e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.2018e-04 - val_accuracy: 1.0000 - val_loss: 8.4174e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.9996e-04 - val_accuracy: 1.0000 - val_loss: 8.2214e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 8.2303e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.0041e-04 - val_accuracy: 1.0000 - val_loss: 7.8515e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Обучение с использованием весов классов\n",
    "print(\"\\n=== НАЧИНАЕМ ОБУЧЕНИЕ ===\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1. Визуализация процесса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Функция для построения графиков обучения\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # График точности\n",
    "    ax1.plot(history.history['accuracy'], label='Обучающая точность')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Валидационная точность')\n",
    "    ax1.set_title('Точность модели')\n",
    "    ax1.set_xlabel('Эпоха')\n",
    "    ax1.set_ylabel('Точность')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # График потерь\n",
    "    ax2.plot(history.history['loss'], label='Обучающие потери')\n",
    "    ax2.plot(history.history['val_loss'], label='Валидационные потери')\n",
    "    ax2.set_title('Потери модели')\n",
    "    ax2.set_xlabel('Эпоха')\n",
    "    ax2.set_ylabel('Потери')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Строим графики\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ОЦЕНКА МОДЕЛИ НА ТЕСТОВЫХ ДАННЫХ ===\n",
      "Точность на тестовых данных: 1.0000\n",
      "Уникальные классы в true: 5\n",
      "Уникальные классы в pred: 5\n",
      "Все классы для отчета: 5\n",
      "\n",
      "Отчет о классификации:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "          Тройники канализационные       1.00      1.00      1.00         6\n",
      "               Тройники по чертежу       1.00      1.00      1.00         3\n",
      "    Тройники стандартные импортные       1.00      1.00      1.00         5\n",
      "Тройники стандартные отечественные       1.00      1.00      1.00        52\n",
      "                Тройники фирменные       1.00      1.00      1.00         7\n",
      "\n",
      "                          accuracy                           1.00        73\n",
      "                         macro avg       1.00      1.00      1.00        73\n",
      "                      weighted avg       1.00      1.00      1.00        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Оценка на тестовых данных\n",
    "print(\"=== ОЦЕНКА МОДЕЛИ НА ТЕСТОВЫХ ДАННЫХ ===\")\n",
    "\n",
    "# Используем правильные классы из label_encoder\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Точность на тестовых данных: {test_accuracy:.4f}\")\n",
    "\n",
    "# Предсказания\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Получаем актуальные классы из данных\n",
    "unique_true = np.unique(y_true_classes)\n",
    "unique_pred = np.unique(y_pred_classes)\n",
    "all_classes = np.union1d(unique_true, unique_pred)\n",
    "\n",
    "print(f\"Уникальные классы в true: {len(unique_true)}\")\n",
    "print(f\"Уникальные классы в pred: {len(unique_pred)}\")\n",
    "print(f\"Все классы для отчета: {len(all_classes)}\")\n",
    "\n",
    "# Создаем правильные названия классов только для существующих классов\n",
    "available_classes = label_encoder.classes_[all_classes] if len(all_classes) <= len(label_encoder.classes_) else [f\"Class_{i}\" for i in all_classes]\n",
    "\n",
    "# Отчет о классификации с правильными labels\n",
    "print(\"\\nОтчет о классификации:\")\n",
    "print(classification_report(\n",
    "    y_true_classes,\n",
    "    y_pred_classes,\n",
    "    labels=all_classes,\n",
    "    target_names=available_classes,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ЗАГРУЗКА МОДЕЛИ КЛАССИФИКАЦИИ ТМЦ ===\n",
      "1. Загрузка параметров модели...\n",
      "   ✓ MAX_NB_WORDS: 10000\n",
      "   ✓ MAX_SEQUENCE_LENGTH: 100\n",
      "   ✓ EMBEDDING_DIM: 100\n",
      "   ✓ vocab_size: 10000\n",
      "2. Загрузка модели Keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZubarevVV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:970: UserWarning: Layer 'conv1d' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Модель загружена: tmc_classification_model_OKPD.h5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3290</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">213,850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m19,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3290\u001b[0m)           │       \u001b[38;5;34m213,850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,237,276</span> (4.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,237,276\u001b[0m (4.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,237,274</span> (4.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,237,274\u001b[0m (4.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Архитектура: None\n",
      "3. Загрузка токенизатора...\n",
      "   ✓ Токенизатор загружен: 63259 слов в словаре\n",
      "4. Загрузка LabelEncoder...\n",
      "   ✓ LabelEncoder загружен: 3290 классов\n",
      "\n",
      "✓ Все компоненты успешно загружены!\n",
      "\n",
      "✓ Глобальные переменные установлены!\n",
      "Доступные переменные: model, tokenizer, label_encoder, preprocess_text\n",
      "Параметры: MAX_SEQUENCE_LENGTH=100, vocab_size=10000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "\n",
    "def load_tmc_classification_model(model_path='tmc_classification_model_OKPD.h5',\n",
    "                                 tokenizer_path='tokenizer_OKPD.pickle',\n",
    "                                 label_encoder_path='label_encoder_OKPD.pickle',\n",
    "                                 params_path='model_params_OKPD.json'):\n",
    "    \"\"\"\n",
    "    Загружает все компоненты модели для классификации ТМЦ после перезапуска ноутбука\n",
    "    \n",
    "    Returns:\n",
    "    - model: загруженная модель\n",
    "    - tokenizer: загруженный токенизатор\n",
    "    - label_encoder: загруженный кодировщик меток\n",
    "    - params: параметры модели\n",
    "    - preprocess_text: функция предобработки текста\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== ЗАГРУЗКА МОДЕЛИ КЛАССИФИКАЦИИ ТМЦ ===\")\n",
    "    \n",
    "    # Функция предобработки текста\n",
    "    def preprocess_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w\\s\\dх×]', ' ', text)\n",
    "        text = re.sub(r'[х×]', ' x ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Загружаем параметры модели\n",
    "        print(\"1. Загрузка параметров модели...\")\n",
    "        with open(params_path, 'r', encoding='utf-8') as f:\n",
    "            params = json.load(f)\n",
    "        \n",
    "        MAX_NB_WORDS = params['MAX_NB_WORDS']\n",
    "        MAX_SEQUENCE_LENGTH = params['MAX_SEQUENCE_LENGTH']\n",
    "        EMBEDDING_DIM = params.get('EMBEDDING_DIM', 64)\n",
    "        vocab_size = params.get('vocab_size', MAX_NB_WORDS)\n",
    "        \n",
    "        print(f\"   ✓ MAX_NB_WORDS: {MAX_NB_WORDS}\")\n",
    "        print(f\"   ✓ MAX_SEQUENCE_LENGTH: {MAX_SEQUENCE_LENGTH}\")\n",
    "        print(f\"   ✓ EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "        print(f\"   ✓ vocab_size: {vocab_size}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Ошибка загрузки параметров: {e}\")\n",
    "        # Значения по умолчанию\n",
    "        MAX_NB_WORDS = 5000\n",
    "        MAX_SEQUENCE_LENGTH = 50\n",
    "        EMBEDDING_DIM = 64\n",
    "        vocab_size = MAX_NB_WORDS\n",
    "        params = {\n",
    "            'MAX_NB_WORDS': MAX_NB_WORDS,\n",
    "            'MAX_SEQUENCE_LENGTH': MAX_SEQUENCE_LENGTH,\n",
    "            'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "            'vocab_size': vocab_size\n",
    "        }\n",
    "        print(\"   ⚠ Используются параметры по умолчанию\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем модель\n",
    "        print(\"2. Загрузка модели Keras...\")\n",
    "        model = load_model(model_path)\n",
    "        print(f\"   ✓ Модель загружена: {model_path}\")\n",
    "        print(f\"   ✓ Архитектура: {model.summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Ошибка загрузки модели: {e}\")\n",
    "        model = None\n",
    "    \n",
    "    try:\n",
    "        # Загружаем токенизатор\n",
    "        print(\"3. Загрузка токенизатора...\")\n",
    "        with open(tokenizer_path, 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        print(f\"   ✓ Токенизатор загружен: {len(tokenizer.word_index)} слов в словаре\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Ошибка загрузки токенизатора: {e}\")\n",
    "        tokenizer = None\n",
    "    \n",
    "    try:\n",
    "        # Загружаем label encoder\n",
    "        print(\"4. Загрузка LabelEncoder...\")\n",
    "        with open(label_encoder_path, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        print(f\"   ✓ LabelEncoder загружен: {len(label_encoder.classes_)} классов\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ Ошибка загрузки LabelEncoder: {e}\")\n",
    "        label_encoder = None\n",
    "    \n",
    "    # Создаем словарь с результатами\n",
    "    components = {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'label_encoder': label_encoder,\n",
    "        'params': params,\n",
    "        'preprocess_text': preprocess_text,\n",
    "        'MAX_NB_WORDS': MAX_NB_WORDS,\n",
    "        'MAX_SEQUENCE_LENGTH': MAX_SEQUENCE_LENGTH,\n",
    "        'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "        'vocab_size': vocab_size\n",
    "    }\n",
    "    \n",
    "    print(\"\\n✓ Все компоненты успешно загружены!\")\n",
    "    return components\n",
    "\n",
    "# Загружаем все компоненты\n",
    "try:\n",
    "    components = load_tmc_classification_model()\n",
    "    \n",
    "    # Распаковываем компоненты в глобальные переменные\n",
    "    model = components['model']\n",
    "    tokenizer = components['tokenizer']\n",
    "    label_encoder = components['label_encoder']\n",
    "    params = components['params']\n",
    "    preprocess_text = components['preprocess_text']\n",
    "    MAX_NB_WORDS = components['MAX_NB_WORDS']\n",
    "    MAX_SEQUENCE_LENGTH = components['MAX_SEQUENCE_LENGTH']\n",
    "    EMBEDDING_DIM = components['EMBEDDING_DIM']\n",
    "    vocab_size = components['vocab_size']\n",
    "    \n",
    "    print(\"\\n✓ Глобальные переменные установлены!\")\n",
    "    print(\"Доступные переменные: model, tokenizer, label_encoder, preprocess_text\")\n",
    "    print(f\"Параметры: MAX_SEQUENCE_LENGTH={MAX_SEQUENCE_LENGTH}, vocab_size={vocab_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Критическая ошибка при загрузке: {e}\")\n",
    "    print(\"⚠ Проверьте наличие файлов:\")\n",
    "    print(\"  - tmc_classification_final_model.h5\")\n",
    "    print(\"  - final_tokenizer.pickle\") \n",
    "    print(\"  - final_label_encoder.pickle\")\n",
    "    print(\"  - model_params.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1. Тестирование загруженной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(results)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Тестируем загруженную модель\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[43mmodel\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, label_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]):\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== ТЕСТИРОВАНИЕ ЗАГРУЖЕННОЙ МОДЕЛИ ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m     test_texts = [\n\u001b[32m     75\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mМасло трансмиссионное LUKOIL GEAR GL-4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mТруба полипропиленовая PPR 20мм\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mКуртка летняя защитная\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mТестовый товар для проверки\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m     ]\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_tmc_category(text, return_details=False):\n",
    "    \"\"\"\n",
    "    Универсальная функция для предсказания категории ТМЦ\n",
    "    \n",
    "    Parameters:\n",
    "    - text: текст для классификации\n",
    "    - return_details: если True, возвращает детальную информацию\n",
    "    \n",
    "    Returns:\n",
    "    - Если return_details=False: (class_code, confidence)\n",
    "    - Если return_details=True: словарь с деталями\n",
    "    \"\"\"\n",
    "    \n",
    "    if model is None or tokenizer is None or label_encoder is None:\n",
    "        raise ValueError(\"Модель не загружена. Вызовите load_tmc_classification_model() сначала.\")\n",
    "    \n",
    "    # Предобработка\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Токенизация\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Обрезаем индексы\n",
    "    padded_sequence = np.clip(padded_sequence, 0, vocab_size - 1)\n",
    "    \n",
    "    # Предсказание\n",
    "    prediction = model.predict(padded_sequence, verbose=0)\n",
    "    predicted_class_idx = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    # Декодируем класс\n",
    "    original_class_idx = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
    "    \n",
    "    if not return_details:\n",
    "        return original_class_idx, round(confidence, 4)\n",
    "    else:\n",
    "        confidence_status = 'ВЫСОКАЯ' if confidence > 0.7 else 'СРЕДНЯЯ' if confidence > 0.5 else 'НИЗКАЯ'\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'processed_text': processed_text,\n",
    "            'class_code': original_class_idx,\n",
    "            'class_name': f\"Class_{original_class_idx}\",\n",
    "            'confidence': round(confidence, 4),\n",
    "            'confidence_status': confidence_status,\n",
    "            'all_predictions': prediction[0]\n",
    "        }\n",
    "\n",
    "def batch_predict_tmc(texts):\n",
    "    \"\"\"\n",
    "    Пакетная классификация списка текстов\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            result = predict_tmc_category(text, return_details=True)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'original_text': text,\n",
    "                'class_code': None,\n",
    "                'class_name': 'ОШИБКА',\n",
    "                'confidence': 0.0,\n",
    "                'confidence_status': f'ОШИБКА: {str(e)}'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Тестируем загруженную модель\n",
    "if all([model is not None, tokenizer is not None, label_encoder is not None]):\n",
    "    print(\"\\n=== ТЕСТИРОВАНИЕ ЗАГРУЖЕННОЙ МОДЕЛИ ===\")\n",
    "    \n",
    "    test_texts = [\n",
    "        \"Масло трансмиссионное LUKOIL GEAR GL-4\",\n",
    "        \"Труба полипропиленовая PPR 20мм\",\n",
    "        \"Куртка летняя защитная\",\n",
    "        \"Тестовый товар для проверки\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        try:\n",
    "            class_code, confidence = predict_tmc_category(text)\n",
    "            print(f\"Текст: '{text}'\")\n",
    "            print(f\"→ Класс: {class_code}, Уверенность: {confidence}\")\n",
    "            print(\"-\" * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка для '{text}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Функция для предсказания на новых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 Функция для предсказания на новых данных из кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестирование предсказаний:\n",
      "============================================================\n",
      "Текст: 'Масло трансмиссионное синтетическое 75W90'\n",
      "Предсказанная категория: Отводы стандартные отечественные\n",
      "Уверенность: 0.6985\n",
      "----------------------------------------\n",
      "Текст: 'Отвод 90-1-114,3х6,3-20 ГОСТ 30753-2001'\n",
      "Предсказанная категория: Отводы стандартные отечественные\n",
      "Уверенность: 0.9992\n",
      "----------------------------------------\n",
      "Текст: 'Куртка летняя защитная'\n",
      "Предсказанная категория: Отводы стандартные отечественные\n",
      "Уверенность: 0.6985\n",
      "----------------------------------------\n",
      "Текст: 'Теплообменник стальной'\n",
      "Предсказанная категория: Отводы стандартные отечественные\n",
      "Уверенность: 0.6985\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_category(text, model, tokenizer, label_encoder, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Функция для предсказания категории нового текста\n",
    "    \"\"\"\n",
    "    # Предобработка текста\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Токенизация и паддинг\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Предсказание\n",
    "    prediction = model.predict(padded_sequence, verbose=0)\n",
    "    predicted_class_idx = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    # Преобразование обратно в текстовую метку\n",
    "    predicted_class = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Тестирование функции на примерах\n",
    "test_texts = [\n",
    "    \"Масло трансмиссионное синтетическое 75W90\",\n",
    "    \"Отвод 90-1-114,3х6,3-20 ГОСТ 30753-2001\",\n",
    "    \"Куртка летняя защитная\",\n",
    "    \"Теплообменник стальной\"\n",
    "]\n",
    "\n",
    "print(\"Тестирование предсказаний:\")\n",
    "print(\"=\" * 60)\n",
    "for text in test_texts:\n",
    "    category, confidence = predict_category(text, model, tokenizer, label_encoder, MAX_SEQUENCE_LENGTH)\n",
    "    print(f\"Текст: '{text}'\")\n",
    "    print(f\"Предсказанная категория: {category}\")\n",
    "    print(f\"Уверенность: {confidence:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 Создание маппинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создан mapping классов:\n",
      "  - Уникальных кодов: 6\n",
      "  - Уникальных названий: 5\n",
      "Примеры mapping:\n",
      "  2013070303000000 → Тройники фирменные\n",
      "  1711030100000000 → Тройники стандартные отечественные\n",
      "  2013070304000000 → Тройники по чертежу\n",
      "  2013070302000000 → Тройники стандартные импортные\n",
      "  2013070301000000 → Тройники стандартные отечественные\n"
     ]
    }
   ],
   "source": [
    "# Создаем mapping между кодами классов и их названиями из исходного датафрейма\n",
    "def create_class_mapping_from_df(df):\n",
    "    \"\"\"\n",
    "    Создает mapping между Hierarchy_MTR_Class и Hierarchy_MTR_Name из исходного датафрейма\n",
    "    \"\"\"\n",
    "    # Берем уникальные пары код-название\n",
    "    class_mapping = df[['Hierarchy_MTR_Class', 'Hierarchy_MTR_Name']].drop_duplicates()\n",
    "    \n",
    "    # Создаем словарь для быстрого доступа\n",
    "    class_code_to_name = dict(zip(class_mapping['Hierarchy_MTR_Class'], class_mapping['Hierarchy_MTR_Name']))\n",
    "    class_name_to_code = dict(zip(class_mapping['Hierarchy_MTR_Name'], class_mapping['Hierarchy_MTR_Class']))\n",
    "    \n",
    "    print(f\"Создан mapping классов:\")\n",
    "    print(f\"  - Уникальных кодов: {len(class_code_to_name)}\")\n",
    "    print(f\"  - Уникальных названий: {len(class_name_to_code)}\")\n",
    "    \n",
    "    # Покажем несколько примеров\n",
    "    print(\"Примеры mapping:\")\n",
    "    for i, (code, name) in enumerate(list(class_code_to_name.items())[:5]):\n",
    "        print(f\"  {code} → {name}\")\n",
    "    \n",
    "    return class_code_to_name, class_name_to_code\n",
    "\n",
    "# Создаем mapping\n",
    "if 'df' in locals():\n",
    "    class_code_to_name, class_name_to_code = create_class_mapping_from_df(df)\n",
    "else:\n",
    "    print(\"⚠ Исходный датафрейм df не найден\")\n",
    "    class_code_to_name, class_name_to_code = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_excel_file_loaded(input_file_path, output_file_path, \n",
    "                              text_column_name=None,\n",
    "                              sheet_name=0):\n",
    "    \"\"\"\n",
    "    Улучшенная функция для классификации Excel файла с загруженной моделью\n",
    "    Сохраняет все исходные колонки в конце результата\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file_path: путь к входному Excel файлу\n",
    "    - output_file_path: путь для сохранения результатов\n",
    "    - text_column_name: название столбца с текстом (если None, определит автоматически)\n",
    "    - sheet_name: название листа или номер листа\n",
    "    \"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        raise ValueError(\"Модель не загружена!\")\n",
    "    \n",
    "    print(f\"=== КЛАССИФИКАЦИЯ EXCEL ФАЙЛА ===\")\n",
    "    print(f\"Входной файл: {input_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        df_input = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "        print(f\"✓ Загружено {len(df_input)} строк из файла\")\n",
    "        print(f\"✓ Исходные колонки: {list(df_input.columns)}\")\n",
    "        \n",
    "        # Определяем столбец с текстом\n",
    "        if text_column_name and text_column_name in df_input.columns:\n",
    "            text_column = text_column_name\n",
    "            print(f\"✓ Используем указанный столбец: '{text_column}'\")\n",
    "        else:\n",
    "            # Автопоиск столбца с текстом\n",
    "            text_columns = []\n",
    "            for col in df_input.columns:\n",
    "                col_lower = col.lower()\n",
    "                if any(keyword in col_lower for keyword in ['наименование', 'name', 'описание', 'description', 'текст', 'text', 'товар', 'product']):\n",
    "                    text_columns.append(col)\n",
    "            \n",
    "            if text_columns:\n",
    "                text_column = text_columns[0]\n",
    "                print(f\"✓ Автоопределен столбец: '{text_column}'\")\n",
    "            else:\n",
    "                text_column = df_input.columns[0]\n",
    "                print(f\"⚠ Столбец не найден, используем первый: '{text_column}'\")\n",
    "        \n",
    "        # Классификация\n",
    "        results = []\n",
    "        for idx, row in df_input.iterrows():\n",
    "            text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "            \n",
    "            if not text.strip():\n",
    "                result_row = {\n",
    "                    'Исходное_наименование': text,\n",
    "                    'Код_класса': None,\n",
    "                    'Наименование_класса': 'НЕ ОПРЕДЕЛЕНО',\n",
    "                    'Уверенность': 0.0,\n",
    "                    'Статус_уверенности': 'ПУСТАЯ_СТРОКА'\n",
    "                }\n",
    "            else:\n",
    "                try:\n",
    "                    class_code, confidence = predict_tmc_category(text)\n",
    "                    status = 'ВЫСОКАЯ' if confidence > 0.7 else 'СРЕДНЯЯ' if confidence > 0.5 else 'НИЗКАЯ'\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': class_code,\n",
    "                        'Наименование_класса': f\"Class_{class_code}\",\n",
    "                        'Уверенность': confidence,\n",
    "                        'Статус_уверенности': status\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': None,\n",
    "                        'Наименование_класса': 'ОШИБКА',\n",
    "                        'Уверенность': 0.0,\n",
    "                        'Статус_уверенности': f'ОШИБКА: {str(e)}'\n",
    "                    }\n",
    "            \n",
    "            # Добавляем все исходные колонки\n",
    "            for col in df_input.columns:\n",
    "                result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "            # Прогресс\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Обработано {idx + 1} строк...\")\n",
    "        \n",
    "        # Создаем DataFrame с результатами\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Переупорядочиваем колонки: сначала результаты классификации, потом исходные данные\n",
    "        classification_columns = [\n",
    "            'Исходное_наименование', \n",
    "            'Код_класса', \n",
    "            'Наименование_класса', \n",
    "            'Уверенность', \n",
    "            'Статус_уверенности'\n",
    "        ]\n",
    "        \n",
    "        # Убедимся, что все колонки классификации существуют\n",
    "        existing_classification_cols = [col for col in classification_columns if col in df_results.columns]\n",
    "        \n",
    "        # Исходные колонки (исключая те, что уже в классификации)\n",
    "        original_columns = [col for col in df_input.columns if col not in existing_classification_cols]\n",
    "        \n",
    "        # Финальный порядок колонок\n",
    "        final_columns = existing_classification_cols + original_columns\n",
    "        \n",
    "        # Применяем порядок колонок\n",
    "        df_results = df_results[final_columns]\n",
    "        \n",
    "        print(f\"✓ Финальный порядок колонок: {len(final_columns)} колонок\")\n",
    "        print(f\"   Колонки классификации: {existing_classification_cols}\")\n",
    "        print(f\"   Исходные колонки: {original_columns}\")\n",
    "        \n",
    "        # Сохраняем\n",
    "        df_results.to_excel(output_file_path, index=False)\n",
    "        print(f\"✓ Результаты сохранены в: {output_file_path}\")\n",
    "        \n",
    "        # Статистика\n",
    "        total = len(df_results)\n",
    "        success = len([r for r in results if r['Статус_уверенности'] not in ['ПУСТАЯ_СТРОКА', 'ОШИБКА']])\n",
    "        \n",
    "        print(f\"\\n📊 СТАТИСТИКА:\")\n",
    "        print(f\"   Всего строк: {total}\")\n",
    "        print(f\"   Успешно классифицировано: {success} ({success/total*100:.1f}%)\")\n",
    "        print(f\"   Пустых строк: {len([r for r in results if r['Статус_уверенности'] == 'ПУСТАЯ_СТРОКА'])}\")\n",
    "        print(f\"   Ошибок: {len([r for r in results if 'ОШИБКА' in r['Статус_уверенности']])}\")\n",
    "        \n",
    "        # Распределение по уверенности\n",
    "        confidence_stats = df_results['Статус_уверенности'].value_counts()\n",
    "        print(f\"\\n🎯 УВЕРЕННОСТЬ ПРЕДСКАЗАНИЙ:\")\n",
    "        for status, count in confidence_stats.items():\n",
    "            if status not in ['ПУСТАЯ_СТРОКА']:\n",
    "                print(f\"   {status}: {count} ({count/total*100:.1f}%)\")\n",
    "        \n",
    "        return df_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при обработке файла: {e}\")\n",
    "        return None\n",
    "def classify_excel_file_loaded_advanced(input_file_path, output_file_path, \n",
    "                                       text_column_name=None,\n",
    "                                       sheet_name=0,\n",
    "                                       include_original_columns=True,\n",
    "                                       custom_column_order=None):\n",
    "    \"\"\"\n",
    "    Расширенная функция для классификации Excel файла с гибкими настройками колонок\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file_path: путь к входному Excel файлу\n",
    "    - output_file_path: путь для сохранения результатов\n",
    "    - text_column_name: название столбца с текстом\n",
    "    - sheet_name: название листа или номер листа\n",
    "    - include_original_columns: включать ли исходные колонки\n",
    "    - custom_column_order: кастомный порядок колонок (список)\n",
    "    \"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        raise ValueError(\"Модель не загружена!\")\n",
    "    \n",
    "    print(f\"=== РАСШИРЕННАЯ КЛАССИФИКАЦИЯ EXCEL ФАЙЛА ===\")\n",
    "    print(f\"Входной файл: {input_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        df_input = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "        print(f\"✓ Загружено {len(df_input)} строк из файла\")\n",
    "        print(f\"✓ Исходные колонки: {list(df_input.columns)}\")\n",
    "        \n",
    "        # Определяем столбец с текстом\n",
    "        if text_column_name and text_column_name in df_input.columns:\n",
    "            text_column = text_column_name\n",
    "            print(f\"✓ Используем указанный столбец: '{text_column}'\")\n",
    "        else:\n",
    "            text_columns = []\n",
    "            for col in df_input.columns:\n",
    "                col_lower = col.lower()\n",
    "                if any(keyword in col_lower for keyword in ['наименование', 'name', 'описание', 'description', 'текст', 'text', 'товар', 'product']):\n",
    "                    text_columns.append(col)\n",
    "            \n",
    "            if text_columns:\n",
    "                text_column = text_columns[0]\n",
    "                print(f\"✓ Автоопределен столбец: '{text_column}'\")\n",
    "            else:\n",
    "                text_column = df_input.columns[0]\n",
    "                print(f\"⚠ Столбец не найден, используем первый: '{text_column}'\")\n",
    "        \n",
    "        # Классификация\n",
    "        results = []\n",
    "        for idx, row in df_input.iterrows():\n",
    "            text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "            \n",
    "            if not text.strip():\n",
    "                result_row = {\n",
    "                    'Исходное_наименование': text,\n",
    "                    'Код_класса': None,\n",
    "                    'Наименование_класса': 'НЕ ОПРЕДЕЛЕНО',\n",
    "                    'Уверенность': 0.0,\n",
    "                    'Статус_уверенности': 'ПУСТАЯ_СТРОКА'\n",
    "                }\n",
    "            else:\n",
    "                try:\n",
    "                    class_code, confidence = predict_tmc_category(text)\n",
    "                    status = 'ВЫСОКАЯ' if confidence > 0.7 else 'СРЕДНЯЯ' if confidence > 0.5 else 'НИЗКАЯ'\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': class_code,\n",
    "                        'Наименование_класса': f\"Class_{class_code}\",\n",
    "                        'Уверенность': confidence,\n",
    "                        'Статус_уверенности': status\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': None,\n",
    "                        'Наименование_класса': 'ОШИБКА',\n",
    "                        'Уверенность': 0.0,\n",
    "                        'Статус_уверенности': f'ОШИБКА: {str(e)}'\n",
    "                    }\n",
    "            \n",
    "            # Добавляем все исходные колонки если нужно\n",
    "            if include_original_columns:\n",
    "                for col in df_input.columns:\n",
    "                    result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Обработано {idx + 1} строк...\")\n",
    "        \n",
    "        # Создаем DataFrame с результатами\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Определяем порядок колонок\n",
    "        if custom_column_order:\n",
    "            # Используем кастомный порядок\n",
    "            available_columns = [col for col in custom_column_order if col in df_results.columns]\n",
    "            missing_columns = [col for col in df_results.columns if col not in available_columns]\n",
    "            final_columns = available_columns + missing_columns\n",
    "            print(f\"✓ Используется кастомный порядок колонок\")\n",
    "            \n",
    "        else:\n",
    "            # Стандартный порядок: результаты классификации + исходные данные\n",
    "            classification_columns = [\n",
    "                'Исходное_наименование', \n",
    "                'Код_класса', \n",
    "                'Наименование_класса', \n",
    "                'Уверенность', \n",
    "                'Статус_уверенности'\n",
    "            ]\n",
    "            \n",
    "            existing_classification_cols = [col for col in classification_columns if col in df_results.columns]\n",
    "            \n",
    "            if include_original_columns:\n",
    "                original_columns = [col for col in df_input.columns if col not in existing_classification_cols]\n",
    "                final_columns = existing_classification_cols + original_columns\n",
    "            else:\n",
    "                final_columns = existing_classification_cols\n",
    "            \n",
    "            print(f\"✓ Стандартный порядок колонок\")\n",
    "        \n",
    "        # Применяем порядок колонок\n",
    "        df_results = df_results[final_columns]\n",
    "        \n",
    "        print(f\"✓ Финальный порядок колонок ({len(final_columns)}):\")\n",
    "        for i, col in enumerate(final_columns[:10], 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "        if len(final_columns) > 10:\n",
    "            print(f\"   ... и еще {len(final_columns) - 10} колонок\")\n",
    "        \n",
    "        # Сохраняем\n",
    "        df_results.to_excel(output_file_path, index=False)\n",
    "        print(f\"✓ Результаты сохранены в: {output_file_path}\")\n",
    "        \n",
    "        # Статистика\n",
    "        total = len(df_results)\n",
    "        success = len([r for r in results if r.get('Статус_уверенности', '') not in ['ПУСТАЯ_СТРОКА', 'ОШИБКА']])\n",
    "        \n",
    "        print(f\"\\n📊 СТАТИСТИКА:\")\n",
    "        print(f\"   Всего строк: {total}\")\n",
    "        print(f\"   Успешно классифицировано: {success} ({success/total*100:.1f}%)\")\n",
    "        \n",
    "        return df_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при обработке файла: {e}\")\n",
    "        return None\n",
    "def classify_excel_file_loaded_correct(input_file_path, output_file_path, \n",
    "                                      text_column_name=None,\n",
    "                                      sheet_name=0):\n",
    "    \"\"\"\n",
    "    Функция для классификации Excel файла с правильными названиями классов из исходного датафрейма\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file_path: путь к входному Excel файлу\n",
    "    - output_file_path: путь для сохранения результатов\n",
    "    - text_column_name: название столбца с текстом\n",
    "    - sheet_name: название листа или номер листа\n",
    "    \"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        raise ValueError(\"Модель не загружена!\")\n",
    "    \n",
    "    if not class_name_to_code:\n",
    "        print(\"⚠ Mapping классов не создан, будут использоваться generic названия\")\n",
    "    \n",
    "    print(f\"=== КЛАССИФИКАЦИЯ EXCEL ФАЙЛА (ПРАВИЛЬНЫЕ НАЗВАНИЯ) ===\")\n",
    "    print(f\"Входной файл: {input_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        df_input = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "        print(f\"✓ Загружено {len(df_input)} строк из файла\")\n",
    "        print(f\"✓ Исходные колонки: {list(df_input.columns)}\")\n",
    "        \n",
    "        # Определяем столбец с текстом\n",
    "        if text_column_name and text_column_name in df_input.columns:\n",
    "            text_column = text_column_name\n",
    "            print(f\"✓ Используем указанный столбец: '{text_column}'\")\n",
    "        else:\n",
    "            text_columns = []\n",
    "            for col in df_input.columns:\n",
    "                col_lower = col.lower()\n",
    "                if any(keyword in col_lower for keyword in ['наименование', 'name', 'описание', 'description', 'текст', 'text', 'товар', 'product']):\n",
    "                    text_columns.append(col)\n",
    "            \n",
    "            if text_columns:\n",
    "                text_column = text_columns[0]\n",
    "                print(f\"✓ Автоопределен столбец: '{text_column}'\")\n",
    "            else:\n",
    "                text_column = df_input.columns[0]\n",
    "                print(f\"⚠ Столбец не найден, используем первый: '{text_column}'\")\n",
    "        \n",
    "        # Классификация\n",
    "        results = []\n",
    "        for idx, row in df_input.iterrows():\n",
    "            text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "            \n",
    "            if not text.strip():\n",
    "                result_row = {\n",
    "                    'Исходное_наименование': text,\n",
    "                    'Код_класса': None,\n",
    "                    'Наименование_класса': 'НЕ ОПРЕДЕЛЕНО',\n",
    "                    'Уверенность': 0.0,\n",
    "                    'Статус_уверенности': 'ПУСТАЯ_СТРОКА'\n",
    "                }\n",
    "            else:\n",
    "                try:\n",
    "                    class_code, confidence = predict_tmc_category(text)\n",
    "                    status = 'ВЫСОКАЯ' if confidence > 0.7 else 'СРЕДНЯЯ' if confidence > 0.5 else 'НИЗКАЯ'\n",
    "                    \n",
    "                    # Получаем правильное название класса из mapping\n",
    "                    class_name = class_name_to_code.get(class_code, f\"Class_{class_code}\")\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': class_code,  # Hierarchy_MTR_Name\n",
    "                        'Наименование_класса': class_name,  # Hierarchy_MTR_Class\n",
    "                        'Уверенность': confidence,\n",
    "                        'Статус_уверенности': status\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': None,\n",
    "                        'Наименование_класса': 'ОШИБКА',\n",
    "                        'Уверенность': 0.0,\n",
    "                        'Статус_уверенности': f'ОШИБКА: {str(e)}'\n",
    "                    }\n",
    "            \n",
    "            # Добавляем все исходные колонки\n",
    "            for col in df_input.columns:\n",
    "                result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "            # Прогресс\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Обработано {idx + 1} строк...\")\n",
    "        \n",
    "        # Создаем DataFrame с результатами\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Переупорядочиваем колонки: сначала результаты классификации, потом исходные данные\n",
    "        classification_columns = [\n",
    "            'Исходное_наименование', \n",
    "            'Код_класса', \n",
    "            'Наименование_класса', \n",
    "            'Уверенность', \n",
    "            'Статус_уверенности'\n",
    "        ]\n",
    "        \n",
    "        # Убедимся, что все колонки классификации существуют\n",
    "        existing_classification_cols = [col for col in classification_columns if col in df_results.columns]\n",
    "        \n",
    "        # Исходные колонки (исключая те, что уже в классификации)\n",
    "        original_columns = [col for col in df_input.columns if col not in existing_classification_cols]\n",
    "        \n",
    "        # Финальный порядок колонок\n",
    "        final_columns = existing_classification_cols + original_columns\n",
    "        \n",
    "        # Применяем порядок колонок\n",
    "        df_results = df_results[final_columns]\n",
    "        \n",
    "        print(f\"✓ Финальный порядок колонок: {len(final_columns)} колонок\")\n",
    "        \n",
    "        # Сохраняем\n",
    "        df_results.to_excel(output_file_path, index=False)\n",
    "        print(f\"✓ Результаты сохранены в: {output_file_path}\")\n",
    "        \n",
    "        # Статистика\n",
    "        total = len(df_results)\n",
    "        success = len([r for r in results if r['Статус_уверенности'] not in ['ПУСТАЯ_СТРОКА', 'ОШИБКА']])\n",
    "        \n",
    "        print(f\"\\n📊 СТАТИСТИКА:\")\n",
    "        print(f\"   Всего строк: {total}\")\n",
    "        print(f\"   Успешно классифицировано: {success} ({success/total*100:.1f}%)\")\n",
    "        print(f\"   Пустых строк: {len([r for r in results if r['Статус_уверенности'] == 'ПУСТАЯ_СТРОКА'])}\")\n",
    "        print(f\"   Ошибок: {len([r for r in results if 'ОШИБКА' in r['Статус_уверенности']])}\")\n",
    "        \n",
    "        # Статистика по классам\n",
    "        class_distribution = df_results['Наименование_класса'].value_counts()\n",
    "        print(f\"\\n🏷️  РАСПРЕДЕЛЕНИЕ ПО КЛАССАМ (топ-10):\")\n",
    "        for class_name, count in class_distribution.head(10).items():\n",
    "            if class_name not in ['НЕ ОПРЕДЕЛЕНО', 'ОШИБКА']:\n",
    "                print(f\"   {class_name}: {count}\")\n",
    "        # Распределение по уверенности\n",
    "        confidence_stats = df_results['Статус_уверенности'].value_counts()\n",
    "        print(f\"\\n🎯 УВЕРЕННОСТЬ ПРЕДСКАЗАНИЙ:\")\n",
    "        for status, count in confidence_stats.items():\n",
    "            if status not in ['ПУСТАЯ_СТРОКА']:\n",
    "                print(f\"   {status}: {count} ({count/total*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "        return df_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при обработке файла: {e}\")\n",
    "        return None\n",
    "def classify_excel_file_with_combined_names(input_file_path, output_file_path, \n",
    "                                          short_name_column=None,\n",
    "                                          full_name_column=None,\n",
    "                                          sheet_name=0):\n",
    "    \"\"\"\n",
    "    Классификация Excel файла с объединением краткого и полного наименования\n",
    "    \"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        raise ValueError(\"Модель не загружена!\")\n",
    "    \n",
    "    print(f\"=== КЛАССИФИКАЦИЯ EXCEL ФАЙЛА (С ОБЪЕДИНЕНИЕМ НАИМЕНОВАНИЙ) ===\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        df_input = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "        print(f\"✓ Загружено {len(df_input)} строк из файла\")\n",
    "        \n",
    "        # Автопоиск колонок с наименованиями\n",
    "        if not short_name_column:\n",
    "            short_name_column = find_column_by_keywords(df_input, ['кратк', 'short', 'name', 'наименован'])\n",
    "        if not full_name_column:\n",
    "            full_name_column = find_column_by_keywords(df_input, ['полн', 'full', 'описан', 'description'])\n",
    "        \n",
    "        print(f\"✓ Колонка краткого наименования: '{short_name_column}'\")\n",
    "        print(f\"✓ Колонка полного наименования: '{full_name_column}'\")\n",
    "        \n",
    "        # Объединяем наименования\n",
    "        def combine_names(row):\n",
    "            short_name = str(row[short_name_column]) if pd.notna(row[short_name_column]) else \"\"\n",
    "            full_name = str(row[full_name_column]) if pd.notna(row[full_name_column]) else \"\"\n",
    "            return f\"{short_name} {full_name}\".strip()\n",
    "        \n",
    "        df_input['combined_text'] = df_input.apply(combine_names, axis=1)\n",
    "        \n",
    "        # Классификация\n",
    "        results = []\n",
    "        for idx, row in df_input.iterrows():\n",
    "            combined_text = row['combined_text']\n",
    "            \n",
    "            if not combined_text.strip():\n",
    "                result_row = {\n",
    "                    'Исходное_краткое_наименование': row[short_name_column] if pd.notna(row[short_name_column]) else \"\",\n",
    "                    'Исходное_полное_наименование': row[full_name_column] if pd.notna(row[full_name_column]) else \"\",\n",
    "                    'Код_класса': None,\n",
    "                    'Наименование_класса': 'НЕ ОПРЕДЕЛЕНО',\n",
    "                    'Уверенность': 0.0,\n",
    "                    'Статус_уверенности': 'ПУСТАЯ_СТРОКА'\n",
    "                }\n",
    "            else:\n",
    "                try:\n",
    "                    class_code, confidence = predict_tmc_category(combined_text)\n",
    "                    status = 'ВЫСОКАЯ' if confidence > 0.7 else 'СРЕДНЯЯ' if confidence > 0.5 else 'НИЗКАЯ'\n",
    "                    \n",
    "                    class_name = class_name_to_code.get(class_code, f\"Class_{class_code}\")\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'Исходное_краткое_наименование': row[short_name_column] if pd.notna(row[short_name_column]) else \"\",\n",
    "                        'Исходное_полное_наименование': row[full_name_column] if pd.notna(row[full_name_column]) else \"\",\n",
    "                        'Код_класса': class_code,\n",
    "                        'Наименование_класса': class_name,\n",
    "                        'Уверенность': confidence,\n",
    "                        'Статус_уверенности': status\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    result_row = {\n",
    "                        'Исходное_краткое_наименование': row[short_name_column] if pd.notna(row[short_name_column]) else \"\",\n",
    "                        'Исходное_полное_наименование': row[full_name_column] if pd.notna(row[full_name_column]) else \"\",\n",
    "                        'Код_класса': None,\n",
    "                        'Наименование_класса': 'ОШИБКА',\n",
    "                        'Уверенность': 0.0,\n",
    "                        'Статус_уверенности': f'ОШИБКА: {str(e)}'\n",
    "                    }\n",
    "            \n",
    "            # Добавляем все исходные колонки\n",
    "            for col in df_input.columns:\n",
    "                if col != 'combined_text':  # Исключаем временную колонку\n",
    "                    result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Обработано {idx + 1} строк...\")\n",
    "        \n",
    "        # Создаем и сохраняем DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.to_excel(output_file_path, index=False)\n",
    "        \n",
    "        print(f\"✓ Результаты сохранены в: {output_file_path}\")\n",
    "        return df_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при обработке файла: {e}\")\n",
    "        return None\n",
    "def find_column_by_keywords(df, keywords):\n",
    "    \"\"\"Находит колонку по ключевым словам в названии\"\"\"\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in keywords):\n",
    "            return col\n",
    "    return df.columns[0]  # Возвращаем первую колонку если не нашли\n",
    "def classify_excel_file_loaded_correct_fixed(input_file_path, output_file_path, \n",
    "                                           text_column_name=None,\n",
    "                                           sheet_name=0):\n",
    "    \"\"\"\n",
    "    Функция для классификации Excel файла с правильными названиями классов\n",
    "    с исправлением отображения числовых значений как текста\n",
    "    \"\"\"\n",
    "    import openpyxl\n",
    "    if model is None:\n",
    "        raise ValueError(\"Модель не загружена!\")\n",
    "    \n",
    "    if not class_code_to_name:\n",
    "        print(\"⚠ Mapping классов не создан, будут использоваться generic названия\")\n",
    "    \n",
    "    print(f\"=== КЛАССИФИКАЦИЯ EXCEL ФАЙЛА (ИСПРАВЛЕННАЯ) ===\")\n",
    "    print(f\"Входной файл: {input_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        df_input = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "        print(f\"✓ Загружено {len(df_input)} строк из файла\")\n",
    "        print(f\"✓ Исходные колонки: {list(df_input.columns)}\")\n",
    "        \n",
    "        # Определяем столбец с текстом\n",
    "        if text_column_name and text_column_name in df_input.columns:\n",
    "            text_column = text_column_name\n",
    "            print(f\"✓ Используем указанный столбец: '{text_column}'\")\n",
    "        else:\n",
    "            text_columns = []\n",
    "            for col in df_input.columns:\n",
    "                col_lower = col.lower()\n",
    "                if any(keyword in col_lower for keyword in ['наименование', 'name', 'описание', 'description', 'текст', 'text', 'товар', 'product']):\n",
    "                    text_columns.append(col)\n",
    "            \n",
    "            if text_columns:\n",
    "                text_column = text_columns[0]\n",
    "                print(f\"✓ Автоопределен столбец: '{text_column}'\")\n",
    "            else:\n",
    "                text_column = df_input.columns[0]\n",
    "                print(f\"⚠ Столбец не найден, используем первый: '{text_column}'\")\n",
    "        \n",
    "        # Классификация\n",
    "        results = []\n",
    "        for idx, row in df_input.iterrows():\n",
    "            text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "            \n",
    "            if not text.strip():\n",
    "                result_row = {\n",
    "                    'Исходное_наименование': text,\n",
    "                    'Код_класса': None,\n",
    "                    'Наименование_класса': 'НЕ ОПРЕДЕЛЕНО',\n",
    "                    'Уверенность': 0.0,\n",
    "                    'Статус_уверенности': 'ПУСТАЯ_СТРОКА'\n",
    "                }\n",
    "            else:\n",
    "                try:\n",
    "                    class_code, confidence = predict_tmc_category(text)\n",
    "                    status = 'ВЫСОКАЯ' if confidence > 0.7 else 'СРЕДНЯЯ' if confidence > 0.5 else 'НИЗКАЯ'\n",
    "                    \n",
    "                    # Получаем правильное название класса из mapping\n",
    "                    class_name = class_name_to_code.get(class_code, f\"Class_{class_code}\")\n",
    "                    \n",
    "                    # Преобразуем в строку чтобы избежать экспоненциальной формы\n",
    "                    class_code_str = str(class_code) if class_code is not None else \"\"\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': class_code_str,  # Преобразуем в строку\n",
    "                        'Наименование_класса': str(class_name) if class_name is not None else \"\",\n",
    "                        'Уверенность': confidence,\n",
    "                        'Статус_уверенности': status\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': None,\n",
    "                        'Наименование_класса': 'ОШИБКА',\n",
    "                        'Уверенность': 0.0,\n",
    "                        'Статус_уверенности': f'ОШИБКА: {str(e)}'\n",
    "                    }\n",
    "            \n",
    "            # Добавляем все исходные колонки\n",
    "            for col in df_input.columns:\n",
    "                result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "            # Прогресс\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Обработано {idx + 1} строк...\")\n",
    "        \n",
    "        # Создаем DataFrame с результатами\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Переупорядочиваем колонки: сначала результаты классификации, потом исходные данные\n",
    "        classification_columns = [\n",
    "            'Исходное_наименование', \n",
    "            'Код_класса', \n",
    "            'Наименование_класса', \n",
    "            'Уверенность', \n",
    "            'Статус_уверенности'\n",
    "        ]\n",
    "        \n",
    "        # Убедимся, что все колонки классификации существуют\n",
    "        existing_classification_cols = [col for col in classification_columns if col in df_results.columns]\n",
    "        \n",
    "        # Исходные колонки (исключая те, что уже в классификации)\n",
    "        original_columns = [col for col in df_input.columns if col not in existing_classification_cols]\n",
    "        \n",
    "        # Финальный порядок колонок\n",
    "        final_columns = existing_classification_cols + original_columns\n",
    "        \n",
    "        # Применяем порядок колонок\n",
    "        df_results = df_results[final_columns]\n",
    "        \n",
    "        print(f\"✓ Финальный порядок колонок: {len(final_columns)} колонок\")\n",
    "        \n",
    "        # Сохраняем с правильным форматированием\n",
    "        print(\"💾 Сохранение с форматированием текста...\")\n",
    "        \n",
    "        # Используем openpyxl для сохранения с форматированием\n",
    "        with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "            df_results.to_excel(writer, sheet_name='Результаты', index=False)\n",
    "            \n",
    "            # Получаем workbook и worksheet для форматирования\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets['Результаты']\n",
    "            \n",
    "            # Форматируем заголовки\n",
    "            header_font = openpyxl.styles.Font(bold=True, color=\"FFFFFF\")\n",
    "            header_fill = openpyxl.styles.PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "            \n",
    "            for cell in worksheet[1]:\n",
    "                cell.font = header_font\n",
    "                cell.fill = header_fill\n",
    "            \n",
    "            # Устанавливаем текстовый формат для колонки 'Код_класса'\n",
    "            from openpyxl.styles import numbers\n",
    "            \n",
    "            # Находим индекс колонки 'Код_класса'\n",
    "            if 'Код_класса' in df_results.columns:\n",
    "                col_idx = df_results.columns.get_loc('Код_класса') + 1  # +1 потому что Excel начинается с 1\n",
    "                col_letter = openpyxl.utils.get_column_letter(col_idx)\n",
    "                \n",
    "                # Применяем текстовый формат ко всем ячейкам в колонке\n",
    "                for row in range(2, len(df_results) + 2):  # +2 потому что заголовок в строке 1\n",
    "                    cell = worksheet[f'{col_letter}{row}']\n",
    "                    cell.number_format = '@'  # Текстовый формат в Excel\n",
    "            \n",
    "            # Авто-ширина колонок\n",
    "            for column in worksheet.columns:\n",
    "                max_length = 0\n",
    "                column_letter = column[0].column_letter\n",
    "                for cell in column:\n",
    "                    try:\n",
    "                        if len(str(cell.value)) > max_length:\n",
    "                            max_length = len(str(cell.value))\n",
    "                    except:\n",
    "                        pass\n",
    "                adjusted_width = min(max_length + 2, 50)\n",
    "                worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        \n",
    "        print(f\"✓ Результаты сохранены в: {output_file_path}\")\n",
    "        \n",
    "        # Статистика\n",
    "        total = len(df_results)\n",
    "        success_count = len([r for r in results if r['Статус_уверенности'] not in ['ПУСТАЯ_СТРОКА', 'ОШИБКА']])\n",
    "        \n",
    "        print(f\"\\n📊 СТАТИСТИКА:\")\n",
    "        print(f\"   Всего строк: {total}\")\n",
    "        print(f\"   Успешно классифицировано: {success_count} ({success_count/total*100:.1f}%)\")\n",
    "        print(f\"   Пустых строк: {len([r for r in results if r['Статус_уверенности'] == 'ПУСТАЯ_СТРОКА'])}\")\n",
    "        print(f\"   Ошибок: {len([r for r in results if 'ОШИБКА' in r['Статус_уверенности']])}\")\n",
    "        \n",
    "        # Статистика по классам\n",
    "        class_distribution = df_results['Наименование_класса'].value_counts()\n",
    "        print(f\"\\n🏷️  РАСПРЕДЕЛЕНИЕ ПО КЛАССАМ (топ-10):\")\n",
    "        for class_name, count in class_distribution.head(10).items():\n",
    "            if class_name not in ['НЕ ОПРЕДЕЛЕНО', 'ОШИБКА']:\n",
    "                print(f\"   {class_name}: {count}\")\n",
    "        \n",
    "        # Распределение по уверенности\n",
    "        confidence_stats = df_results['Статус_уверенности'].value_counts()\n",
    "        print(f\"\\n🎯 УВЕРЕННОСТЬ ПРЕДСКАЗАНИЙ:\")\n",
    "        for status, count in confidence_stats.items():\n",
    "            if status not in ['ПУСТАЯ_СТРОКА']:\n",
    "                print(f\"   {status}: {count} ({count/total*100:.1f}%)\")\n",
    "\n",
    "        return df_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при обработке файла: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мэппинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Единичное предсказание с мэппингом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ТЕСТ ОБНОВЛЕННОЙ КЛАССИФИКАЦИИ ===\n",
      "Текст: 'Подшипник редуктора вентилятора градирни QVRC2-CUN Hansen артикул 6009'\n",
      "→ Код класса: Перчатки, рукавицы (варежки) и митенки трикотажные или вязаные\n",
      "→ Название класса: 14.19.13.000\n",
      "→ Уверенность: 0.29899999499320984 (НИЗКАЯ)\n",
      "------------------------------------------------------------\n",
      "Текст: 'Кислота гидроксифенилпропионовая Alfa Aesar B20412'\n",
      "→ Код класса: Спиртосодержащая непищевая продукция\n",
      "→ Название класса: 20.59.59.100\n",
      "→ Уверенность: 0.21960000693798065 (НИЗКАЯ)\n",
      "------------------------------------------------------------\n",
      "Текст: 'Кислота олеиновая (кг)'\n",
      "→ Код класса: Жиры и масла растительные, химически модифицированные\n",
      "→ Название класса: 20.59.20.120\n",
      "→ Уверенность: 0.8461999893188477 (ВЫСОКАЯ)\n",
      "------------------------------------------------------------\n",
      "Текст: 'Кислота аскорбиновая (L-e300)'\n",
      "→ Код класса: Дистилляты легкие, не включенные в другие группировки\n",
      "→ Название класса: 19.20.23.190\n",
      "→ Уверенность: 0.12439999729394913 (НИЗКАЯ)\n",
      "------------------------------------------------------------\n",
      "Текст: 'Кислота аскорбиновая чда ТУКОМП 2-723'\n",
      "→ Код класса: Части прочего электрического оборудования для автотранспортных средств и мотоциклов\n",
      "→ Название класса: 29.31.30.000\n",
      "→ Уверенность: 0.6599000096321106 (СРЕДНЯЯ)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_tmc_category_detailed(text):\n",
    "    \"\"\"\n",
    "    Детальная классификация с правильными названиями классов\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        raise ValueError(\"Модель не загружена!\")\n",
    "    \n",
    "    # Предобработка\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Токенизация\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Обрезаем индексы\n",
    "    padded_sequence = np.clip(padded_sequence, 0, vocab_size - 1)\n",
    "    \n",
    "    # Предсказание\n",
    "    prediction = model.predict(padded_sequence, verbose=0)\n",
    "    predicted_class_idx = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    # Декодируем класс\n",
    "    original_class_idx = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
    "    \n",
    "    # Получаем правильное название класса\n",
    "    class_name = class_name_to_code.get(original_class_idx, f\"Class_{original_class_idx}\")\n",
    "    confidence_status = 'ВЫСОКАЯ' if confidence > 0.7 else 'СРЕДНЯЯ' if confidence > 0.5 else 'НИЗКАЯ'\n",
    "    \n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'class_code': original_class_idx,  # Hierarchy_MTR_Name\n",
    "        'class_name': class_name,          # Hierarchy_MTR_Class\n",
    "        'confidence': round(confidence, 4),\n",
    "        'confidence_status': confidence_status\n",
    "    }\n",
    "\n",
    "# Тестируем обновленную функцию\n",
    "if all([model is not None, tokenizer is not None, label_encoder is not None]):\n",
    "    print(\"\\n=== ТЕСТ ОБНОВЛЕННОЙ КЛАССИФИКАЦИИ ===\")\n",
    "    \n",
    "    test_texts = [\n",
    "        \"Подшипник редуктора вентилятора градирни QVRC2-CUN Hansen артикул 6009\",\n",
    "        \"Кислота гидроксифенилпропионовая Alfa Aesar B20412\",\n",
    "        \"Кислота олеиновая (кг)\",\n",
    "        \"Кислота аскорбиновая (L-e300)\",\n",
    "        \"Кислота аскорбиновая чда ТУКОМП 2-723\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        try:\n",
    "            result = predict_tmc_category_detailed(text)\n",
    "            print(f\"Текст: '{text}'\")\n",
    "            print(f\"→ Код класса: {result['class_code']}\")\n",
    "            print(f\"→ Название класса: {result['class_name']}\")\n",
    "            print(f\"→ Уверенность: {result['confidence']} ({result['confidence_status']})\")\n",
    "            print(\"-\" * 60)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка для '{text}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предсказание с мэппингом из EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПРИМЕР ИСПОЛЬЗОВАНИЯ ===\n",
      "=== КЛАССИФИКАЦИЯ EXCEL ФАЙЛА (ИСПРАВЛЕННАЯ) ===\n",
      "Входной файл: to_class_troynilki.xlsx\n",
      "✓ Загружено 4381 строк из файла\n",
      "✓ Исходные колонки: ['Hierarchy_MTR_Class', 'Hierarchy_MTR_Name', 'CSCD_ID', 'FULL_NAME/ru_RU']\n",
      "✓ Используем указанный столбец: 'FULL_NAME/ru_RU'\n",
      "Обработано 100 строк...\n",
      "Обработано 200 строк...\n",
      "Обработано 300 строк...\n",
      "Обработано 400 строк...\n",
      "Обработано 500 строк...\n",
      "Обработано 600 строк...\n",
      "Обработано 700 строк...\n",
      "Обработано 800 строк...\n",
      "Обработано 900 строк...\n",
      "Обработано 1000 строк...\n",
      "Обработано 1100 строк...\n",
      "Обработано 1200 строк...\n",
      "Обработано 1300 строк...\n",
      "Обработано 1400 строк...\n",
      "Обработано 1500 строк...\n",
      "Обработано 1600 строк...\n",
      "Обработано 1700 строк...\n",
      "Обработано 1800 строк...\n",
      "Обработано 1900 строк...\n",
      "Обработано 2000 строк...\n",
      "Обработано 2100 строк...\n",
      "Обработано 2200 строк...\n",
      "Обработано 2300 строк...\n",
      "Обработано 2400 строк...\n",
      "Обработано 2500 строк...\n",
      "Обработано 2600 строк...\n",
      "Обработано 2700 строк...\n",
      "Обработано 2800 строк...\n",
      "Обработано 2900 строк...\n",
      "Обработано 3000 строк...\n",
      "Обработано 3100 строк...\n",
      "Обработано 3200 строк...\n",
      "Обработано 3300 строк...\n",
      "Обработано 3400 строк...\n",
      "Обработано 3500 строк...\n",
      "Обработано 3600 строк...\n",
      "Обработано 3700 строк...\n",
      "Обработано 3800 строк...\n",
      "Обработано 3900 строк...\n",
      "Обработано 4000 строк...\n",
      "Обработано 4100 строк...\n",
      "Обработано 4200 строк...\n",
      "Обработано 4300 строк...\n",
      "✓ Финальный порядок колонок: 9 колонок\n",
      "💾 Сохранение с форматированием текста...\n",
      "✓ Результаты сохранены в: результат_troynilki.xlsx\n",
      "\n",
      "📊 СТАТИСТИКА:\n",
      "   Всего строк: 4381\n",
      "   Успешно классифицировано: 4381 (100.0%)\n",
      "   Пустых строк: 0\n",
      "   Ошибок: 0\n",
      "\n",
      "🏷️  РАСПРЕДЕЛЕНИЕ ПО КЛАССАМ (топ-10):\n",
      "   2013070301000000: 2318\n",
      "   2013071410000000: 1062\n",
      "   2013070302000000: 863\n",
      "   2013070303000000: 89\n",
      "   2013070304000000: 49\n",
      "\n",
      "🎯 УВЕРЕННОСТЬ ПРЕДСКАЗАНИЙ:\n",
      "   ВЫСОКАЯ: 3378 (77.1%)\n",
      "   СРЕДНЯЯ: 698 (15.9%)\n",
      "   НИЗКАЯ: 305 (7.0%)\n",
      "\n",
      "Первые 5 строк результата:\n",
      "                               Исходное_наименование  \\\n",
      "0        Тройник равнопроходной 1/2\", 12 мм 8.2.14.5   \n",
      "1  Тройник 720х10-109х6-17ГС ТУ 1468-001-61257374...   \n",
      "2  Тройник 720х10-219х9-17ГС ТУ 1468-001-61257374...   \n",
      "3                    Тройник равнопроходной 2.1.26.7   \n",
      "4        Тройник бесшовный равнопроходной ASME B16.9   \n",
      "\n",
      "                           Код_класса Наименование_класса  Уверенность  \n",
      "0      Тройники стандартные импортные    2013070302000000       0.6107  \n",
      "1  Тройники стандартные отечественные    2013070301000000       0.9823  \n",
      "2  Тройники стандартные отечественные    2013070301000000       0.9409  \n",
      "3      Тройники стандартные импортные    2013070302000000       0.6168  \n",
      "4      Тройники стандартные импортные    2013070302000000       0.9995  \n"
     ]
    }
   ],
   "source": [
    "# Пример использования обновленной функции\n",
    "if all([model is not None, tokenizer is not None, label_encoder is not None]):\n",
    "    print(\"=== ПРИМЕР ИСПОЛЬЗОВАНИЯ ===\")\n",
    "    \n",
    "    result = classify_excel_file_loaded_correct_fixed(\n",
    "        input_file_path='to_class_troynilki.xlsx',\n",
    "        output_file_path='результат_troynilki.xlsx',\n",
    "        text_column_name='FULL_NAME/ru_RU'\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"\\nПервые 5 строк результата:\")\n",
    "        print(result[['Исходное_наименование', 'Код_класса', 'Наименование_класса', 'Уверенность']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Сохранение модели и компонентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель и компоненты успешно сохранены!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Сохраняем модель\n",
    "model.save('tmc_classification_model_OKPD.h5')\n",
    "model.save('my_model_OKPD.keras')\n",
    "\n",
    "# Сохраняем токенизатор\n",
    "with open('tokenizer_OKPD.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Сохраняем label encoder\n",
    "with open('label_encoder_OKPD.pickle', 'wb') as handle:\n",
    "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Сохраняем параметры\n",
    "model_params = {\n",
    "    'MAX_NB_WORDS': MAX_NB_WORDS,\n",
    "    'MAX_SEQUENCE_LENGTH': MAX_SEQUENCE_LENGTH,\n",
    "    'EMBEDDING_DIM': EMBEDDING_DIM\n",
    "}\n",
    "\n",
    "with open('model_params_OKPD.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_params, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Модель и компоненты успешно сохранены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Дообучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка готовности к дообучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fine_tuning_readiness():\n",
    "    \"\"\"\n",
    "    Проверяет, готова ли система к дообучению\n",
    "    \"\"\"\n",
    "    print(\"=== ПРОВЕРКА ГОТОВНОСТИ К ДООБУЧЕНИЮ ===\")\n",
    "    \n",
    "    components_ok = True\n",
    "    \n",
    "    # Проверяем модель\n",
    "    if model is None:\n",
    "        print(\"✗ Модель не загружена\")\n",
    "        components_ok = False\n",
    "    else:\n",
    "        try:\n",
    "            # Правильный способ получить выходную shape\n",
    "            output_shape = model.output_shape\n",
    "            if output_shape:\n",
    "                num_classes = output_shape[-1] if isinstance(output_shape, tuple) else output_shape[0][-1]\n",
    "                print(f\"✓ Модель загружена: {num_classes} выходов\")\n",
    "            else:\n",
    "                print(\"✓ Модель загружена (не удалось определить выходную shape)\")\n",
    "        except Exception as e:\n",
    "            print(f\"✓ Модель загружена (ошибка определения shape: {e})\")\n",
    "    \n",
    "    # Проверяем токенизатор\n",
    "    if tokenizer is None:\n",
    "        print(\"✗ Токенизатор не загружен\")\n",
    "        components_ok = False\n",
    "    else:\n",
    "        print(f\"✓ Токенизатор загружен: {len(tokenizer.word_index)} слов\")\n",
    "    \n",
    "    # Проверяем label encoder\n",
    "    if label_encoder is None:\n",
    "        print(\"✗ LabelEncoder не загружен\")\n",
    "        components_ok = False\n",
    "    else:\n",
    "        print(f\"✓ LabelEncoder загружен: {len(label_encoder.classes_)} классов\")\n",
    "    \n",
    "    # Проверяем mapping классов\n",
    "    if not class_code_to_name:\n",
    "        print(\"⚠ Mapping классов пуст\")\n",
    "    else:\n",
    "        print(f\"✓ Mapping классов: {len(class_code_to_name)} пар\")\n",
    "    \n",
    "    # Проверяем параметры\n",
    "    if 'MAX_SEQUENCE_LENGTH' in locals():\n",
    "        print(f\"✓ MAX_SEQUENCE_LENGTH: {MAX_SEQUENCE_LENGTH}\")\n",
    "    else:\n",
    "        print(\"⚠ MAX_SEQUENCE_LENGTH не определен\")\n",
    "    \n",
    "    if 'vocab_size' in locals():\n",
    "        print(f\"✓ vocab_size: {vocab_size}\")\n",
    "    else:\n",
    "        print(\"⚠ vocab_size не определен\")\n",
    "    \n",
    "    if components_ok:\n",
    "        print(\"\\n✅ Система готова к дообучению!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\n❌ Система не готова к дообучению!\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Проверяем готовность\n",
    "check_fine_tuning_readiness()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Детальная информация о модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info():\n",
    "    \"\"\"\n",
    "    Выводит детальную информацию о загруженной модели\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"Модель не загружена\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== ИНФОРМАЦИЯ О МОДЕЛИ ===\")\n",
    "    \n",
    "    try:\n",
    "        # Основная информация\n",
    "        print(f\"Название модели: {model.name}\")\n",
    "        print(f\"Количество слоев: {len(model.layers)}\")\n",
    "        \n",
    "        # Информация о входах и выходах\n",
    "        if hasattr(model, 'input_shape'):\n",
    "            print(f\"Входная shape: {model.input_shape}\")\n",
    "        if hasattr(model, 'output_shape'):\n",
    "            print(f\"Выходная shape: {model.output_shape}\")\n",
    "        \n",
    "        # Информация о слоях\n",
    "        print(\"\\nСлои модели:\")\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            layer_type = type(layer).__name__\n",
    "            try:\n",
    "                output_shape = layer.output_shape\n",
    "                config = layer.get_config()\n",
    "                print(f\"  {i+1:2d}. {layer_type:15} -> {output_shape}\")\n",
    "            except:\n",
    "                print(f\"  {i+1:2d}. {layer_type:15} -> не удалось получить информацию\")\n",
    "        \n",
    "        # Информация о последнем слое (выходном)\n",
    "        if model.layers:\n",
    "            last_layer = model.layers[-1]\n",
    "            print(f\"\\nВыходной слой: {type(last_layer).__name__}\")\n",
    "            if hasattr(last_layer, 'units'):\n",
    "                print(f\"Количество нейронов: {last_layer.units}\")\n",
    "            if hasattr(last_layer, 'activation'):\n",
    "                print(f\"Функция активации: {last_layer.activation.__name__}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении информации о модели: {e}\")\n",
    "    \n",
    "    # Дополнительная информация о данных\n",
    "    print(f\"\\n=== ИНФОРМАЦИЯ О ДАННЫХ ===\")\n",
    "    print(f\"MAX_SEQUENCE_LENGTH: {MAX_SEQUENCE_LENGTH}\")\n",
    "    print(f\"vocab_size: {vocab_size}\")\n",
    "    print(f\"Количество классов в LabelEncoder: {len(label_encoder.classes_) if label_encoder else 'N/A'}\")\n",
    "\n",
    "# Получаем информацию о модели\n",
    "get_model_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полное дообучение (использовать это)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_preserve_weights_fixed_v2(new_data_file_path, \n",
    "                                       short_name_column='SHORT_NAME/ru_RU',\n",
    "                                       full_name_column='FULL_NAME/ru_RU',\n",
    "                                       class_code_column='Hierarchy_MTR_Name', \n",
    "                                       class_name_column='Hierarchy_MTR_Class',\n",
    "                                       epochs=10,\n",
    "                                       batch_size=16,\n",
    "                                       validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Исправленная версия с правильной инициализацией весов\n",
    "    \"\"\"\n",
    "    global model, label_encoder, class_code_to_name\n",
    "    \n",
    "    print(\"=== ДООБУЧЕНИЕ С СОХРАНЕНИЕМ ВЕСОВ (ВЕРСИЯ 2) ===\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Сохраняем веса текущей модели\n",
    "        print(\"1. Сохранение весов текущей модели...\")\n",
    "        old_weights = model.get_weights()\n",
    "        old_num_classes = len(label_encoder.classes_)\n",
    "        print(f\"✓ Сохранены веса для {old_num_classes} классов\")\n",
    "        print(f\"✓ Количество тензоров весов: {len(old_weights)}\")\n",
    "        \n",
    "        # 2. Загружаем новые данные\n",
    "        print(\"2. Загрузка новых данных...\")\n",
    "        if new_data_file_path.endswith('.xlsx'):\n",
    "            new_df = pd.read_excel(new_data_file_path)\n",
    "        else:\n",
    "            new_df = pd.read_csv(new_data_file_path, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        print(f\"✓ Загружено {len(new_df)} примеров\")\n",
    "        \n",
    "        # 3. Объединяем наименования\n",
    "        def combine_names(row):\n",
    "            short_name = str(row[short_name_column]) if pd.notna(row[short_name_column]) else \"\"\n",
    "            full_name = str(row[full_name_column]) if pd.notna(row[full_name_column]) else \"\"\n",
    "            return f\"{short_name} {full_name}\".strip()\n",
    "        \n",
    "        new_df['combined_text'] = new_df.apply(combine_names, axis=1)\n",
    "        \n",
    "        # 4. Анализируем новые классы\n",
    "        new_classes = new_df[class_code_column].unique()\n",
    "        existing_classes = set(label_encoder.classes_)\n",
    "        new_unique_classes = set(new_classes) - existing_classes\n",
    "        \n",
    "        if not new_unique_classes:\n",
    "            print(\"ℹ️ Новых классов не обнаружено, дообучаем на существующих...\")\n",
    "            return fine_tune_existing_classes_only(new_data_file_path, epochs, batch_size)\n",
    "        \n",
    "        print(f\"⚠ Найдены новые классы: {len(new_unique_classes)}\")\n",
    "        print(f\"   Новые классы: {list(new_unique_classes)}\")\n",
    "        \n",
    "        # 5. Создаем новую модель с правильным количеством классов\n",
    "        print(\"3. Создание модели с увеличенным выходным слоем...\")\n",
    "        new_num_classes = old_num_classes + len(new_unique_classes)\n",
    "        \n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "        \n",
    "        # Создаем модель с ТОЧНО ТАКОЙ ЖЕ архитектурой, кроме выходного слоя\n",
    "        new_model = Sequential()\n",
    "        \n",
    "        # Добавляем слои вручную чтобы контролировать архитектуру\n",
    "        new_model.add(Embedding(vocab_size, 64, input_length=MAX_SEQUENCE_LENGTH))\n",
    "        new_model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "        new_model.add(MaxPooling1D(2))\n",
    "        new_model.add(Conv1D(32, 3, activation='relu', padding='same'))\n",
    "        new_model.add(MaxPooling1D(2))\n",
    "        new_model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.1))\n",
    "        new_model.add(Dense(32, activation='relu'))\n",
    "        new_model.add(Dropout(0.3))\n",
    "        new_model.add(Dense(new_num_classes, activation='softmax', name='output_layer'))\n",
    "        \n",
    "        # ВАЖНО: Инициализируем веса модели, вызвав build или сделав предсказание\n",
    "        print(\"4. Инициализация весов новой модели...\")\n",
    "        new_model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
    "        \n",
    "        # Компилируем модель\n",
    "        new_model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Получаем веса новой модели (теперь они должны быть инициализированы)\n",
    "        new_weights = new_model.get_weights()\n",
    "        print(f\"✓ Новая модель создана: {new_num_classes} классов\")\n",
    "        print(f\"✓ Количество тензоров в новой модели: {len(new_weights)}\")\n",
    "        \n",
    "        # Выводим информацию о весах для отладки\n",
    "        print(\"Информация о весах для отладки:\")\n",
    "        for i, w in enumerate(new_weights):\n",
    "            print(f\"  Тензор {i}: форма {w.shape}\")\n",
    "        \n",
    "        # 6. Переносим веса слой за слоем\n",
    "        print(\"5. Перенос весов...\")\n",
    "        \n",
    "        # Определяем какие слои можно перенести (все кроме выходного)\n",
    "        num_layers_to_transfer = min(len(old_weights) - 2, len(new_weights) - 2)\n",
    "        \n",
    "        for i in range(num_layers_to_transfer):\n",
    "            if i < len(old_weights) and i < len(new_weights):\n",
    "                old_shape = old_weights[i].shape\n",
    "                new_shape = new_weights[i].shape\n",
    "                \n",
    "                if old_shape == new_shape:\n",
    "                    new_weights[i] = old_weights[i]\n",
    "                    print(f\"   ✓ Слой {i}: {old_shape} → {new_shape}\")\n",
    "                else:\n",
    "                    print(f\"   ⚠ Слой {i}: форма не совпадает {old_shape} vs {new_shape}\")\n",
    "        \n",
    "        # 7. Особенная обработка выходного слоя\n",
    "        print(\"6. Обработка выходного слоя...\")\n",
    "        \n",
    "        # В старой модели выходной слой - последние 2 тензора\n",
    "        old_output_weights = old_weights[-2]  # weights: (32, 1502)\n",
    "        old_output_bias = old_weights[-1]     # bias: (1502,)\n",
    "        \n",
    "        print(f\"   Старый выходной слой: weights {old_output_weights.shape}, bias {old_output_bias.shape}\")\n",
    "        \n",
    "        # В новой модели выходной слой тоже последние 2 тензора\n",
    "        if len(new_weights) >= 2:\n",
    "            new_output_weights = new_weights[-2]  # weights: (32, 1503)\n",
    "            new_output_bias = new_weights[-1]     # bias: (1503,)\n",
    "            print(f\"   Новый выходной слой до: weights {new_output_weights.shape}, bias {new_output_bias.shape}\")\n",
    "        else:\n",
    "            print(\"❌ Ошибка: в новой модели недостаточно тензоров весов\")\n",
    "            return False\n",
    "        \n",
    "        # Создаем новые веса для выходного слоя\n",
    "        # weights: (input_dim, output_dim) = (32, new_num_classes)\n",
    "        new_output_weights_adapted = np.zeros((old_output_weights.shape[0], new_num_classes))\n",
    "        new_output_bias_adapted = np.zeros(new_num_classes)\n",
    "        \n",
    "        # Копируем старые веса в начало\n",
    "        new_output_weights_adapted[:, :old_num_classes] = old_output_weights\n",
    "        new_output_bias_adapted[:old_num_classes] = old_output_bias\n",
    "        \n",
    "        # Инициализируем веса для новых классов маленькими случайными значениями\n",
    "        for i in range(old_num_classes, new_num_classes):\n",
    "            new_output_weights_adapted[:, i] = np.random.normal(0, 0.01, old_output_weights.shape[0])\n",
    "            new_output_bias_adapted[i] = 0.0\n",
    "        \n",
    "        # Заменяем веса в новом массиве\n",
    "        new_weights[-2] = new_output_weights_adapted\n",
    "        new_weights[-1] = new_output_bias_adapted\n",
    "        \n",
    "        print(f\"   Новый выходной слой после: weights {new_weights[-2].shape}, bias {new_weights[-1].shape}\")\n",
    "        \n",
    "        # 8. Устанавливаем веса в новую модель\n",
    "        new_model.set_weights(new_weights)\n",
    "        \n",
    "        # 9. Обновляем label_encoder\n",
    "        all_classes = np.concatenate([label_encoder.classes_, list(new_unique_classes)])\n",
    "        label_encoder.fit(all_classes)\n",
    "        \n",
    "        model = new_model\n",
    "        print(f\"✓ Модель адаптирована: {old_num_classes} → {new_num_classes} классов\")\n",
    "        \n",
    "        # 10. Подготовка данных для дообучения\n",
    "        print(\"7. Подготовка данных...\")\n",
    "        new_texts = new_df['combined_text'].apply(preprocess_text).tolist()\n",
    "        new_labels_encoded = label_encoder.transform(new_df[class_code_column])\n",
    "        new_y = to_categorical(new_labels_encoded, num_classes=len(label_encoder.classes_))\n",
    "        \n",
    "        new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "        new_X = pad_sequences(new_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        new_X = np.clip(new_X, 0, vocab_size - 1)\n",
    "        \n",
    "        print(f\"✓ Данные подготовлены: X{new_X.shape}, y{new_y.shape}\")\n",
    "        \n",
    "        # 11. Обновляем mapping\n",
    "        new_mapping = new_df[[class_code_column, class_name_column]].drop_duplicates()\n",
    "        for _, row in new_mapping.iterrows():\n",
    "            class_code_to_name[row[class_code_column]] = row[class_name_column]\n",
    "        \n",
    "        # 12. Дообучение с маленьким learning rate\n",
    "        print(\"8. Дообучение...\")\n",
    "        \n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0001),  # Очень маленький LR\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            new_X, new_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1,\n",
    "            validation_split=validation_split\n",
    "        )\n",
    "        \n",
    "        # 13. Сохраняем результаты\n",
    "        model.save('tmc_classification_final_model.h5')\n",
    "        with open('final_label_encoder.pickle', 'wb') as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "        \n",
    "        print(\"✅ Модель дообучена и сохранена!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВЕРСИЯ ЕСЛИ НЕТ НОВЫХ КЛАССОВ\n",
    "def fine_tune_existing_classes_only(new_data_file_path, epochs=5, batch_size=8):\n",
    "    \"\"\"\n",
    "    Дообучение только на существующих классах (без новых классов)\n",
    "    \"\"\"\n",
    "    global model, label_encoder, class_code_to_name\n",
    "    \n",
    "    print(\"=== ДООБУЧЕНИЕ НА СУЩЕСТВУЮЩИХ КЛАССАХ ===\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        if new_data_file_path.endswith('.xlsx'):\n",
    "            new_df = pd.read_excel(new_data_file_path)\n",
    "        else:\n",
    "            new_df = pd.read_csv(new_data_file_path, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        # Фильтруем только существующие классы\n",
    "        existing_classes = set(label_encoder.classes_)\n",
    "        new_df_filtered = new_df[new_df['Hierarchy_MTR_Name'].isin(existing_classes)]\n",
    "        \n",
    "        if len(new_df_filtered) == 0:\n",
    "            print(\"ℹ️ Нет данных с существующими классами\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"✓ Используется {len(new_df_filtered)} примеров с существующими классами\")\n",
    "        \n",
    "        # Объединяем наименования\n",
    "        def combine_names(row):\n",
    "            short_name = str(row['SHORT_NAME/ru_RU']) if pd.notna(row['SHORT_NAME/ru_RU']) else \"\"\n",
    "            full_name = str(row['FULL_NAME/ru_RU']) if pd.notna(row['FULL_NAME/ru_RU']) else \"\"\n",
    "            return f\"{short_name} {full_name}\".strip()\n",
    "        \n",
    "        new_df_filtered['combined_text'] = new_df_filtered.apply(combine_names, axis=1)\n",
    "        \n",
    "        # Подготовка данных\n",
    "        new_texts = new_df_filtered['combined_text'].apply(preprocess_text).tolist()\n",
    "        new_labels_encoded = label_encoder.transform(new_df_filtered['Hierarchy_MTR_Name'])\n",
    "        new_y = to_categorical(new_labels_encoded, num_classes=len(label_encoder.classes_))\n",
    "        \n",
    "        new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "        new_X = pad_sequences(new_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        new_X = np.clip(new_X, 0, vocab_size - 1)\n",
    "        \n",
    "        print(f\"✓ Данные подготовлены: X{new_X.shape}, y{new_y.shape}\")\n",
    "        \n",
    "        # Дообучение\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            new_X, new_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Сохраняем\n",
    "        model.save('tmc_classification_final_model.h5')\n",
    "        print(\"✅ Модель дообучена на существующих классах!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества дообучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_after_fine_tuning(test_file_path,\n",
    "                              short_name_column='SHORT_NAME/ru_RU',\n",
    "                              full_name_column='FULL_NAME/ru_RU',\n",
    "                              class_code_column='Hierarchy_MTR_Name'):\n",
    "    \"\"\"\n",
    "    Оценка качества модели после дообучения с объединением наименований\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== ОЦЕНКА КАЧЕСТВА ПОСЛЕ ДООБУЧЕНИЯ ===\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем тестовые данные\n",
    "        test_df = pd.read_excel(test_file_path) if test_file_path.endswith('.xlsx') else pd.read_csv(test_file_path, sep='\\t')\n",
    "        \n",
    "        # Объединяем краткое и полное наименование\n",
    "        def combine_names(row):\n",
    "            short_name = str(row[short_name_column]) if pd.notna(row[short_name_column]) else \"\"\n",
    "            full_name = str(row[full_name_column]) if pd.notna(row[full_name_column]) else \"\"\n",
    "            return f\"{short_name} {full_name}\".strip()\n",
    "        \n",
    "        test_df['combined_text'] = test_df.apply(combine_names, axis=1)\n",
    "        \n",
    "        # Подготовка данных\n",
    "        test_texts = test_df['combined_text'].apply(preprocess_text).tolist()\n",
    "        test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "        test_X = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        test_X = np.clip(test_X, 0, vocab_size - 1)\n",
    "        \n",
    "        # Кодируем метки\n",
    "        test_labels_encoded = label_encoder.transform(test_df[class_code_column])\n",
    "        test_y = to_categorical(test_labels_encoded, num_classes=len(label_encoder.classes_))\n",
    "        \n",
    "        # Оценка модели\n",
    "        test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "        \n",
    "        print(f\"📊 РЕЗУЛЬТАТЫ ОЦЕНКИ:\")\n",
    "        print(f\"   Точность: {test_accuracy:.4f}\")\n",
    "        print(f\"   Потери: {test_loss:.4f}\")\n",
    "        print(f\"   Тестовых примеров: {len(test_X)}\")\n",
    "        print(f\"   Количество классов: {len(label_encoder.classes_)}\")\n",
    "        \n",
    "        # Предсказания для детального анализа\n",
    "        y_pred = model.predict(test_X, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Отчет по классификации\n",
    "        from sklearn.metrics import classification_report\n",
    "        \n",
    "        print(f\"\\n📈 ДЕТАЛЬНЫЙ ОТЧЕТ:\")\n",
    "        print(classification_report(test_labels_encoded, y_pred_classes, \n",
    "                                  target_names=[str(cls) for cls in label_encoder.classes_[:10]],\n",
    "                                  zero_division=0))\n",
    "        \n",
    "        return test_accuracy, test_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка оценки: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "evaluate_after_fine_tuning(test_file_path='re_training_tmc_1.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция диагностики возможности дообучения на совместимости классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_fine_tuning_issue(new_data_file_path,\n",
    "                              short_name_column='SHORT_NAME/ru_RU',\n",
    "                              full_name_column='FULL_NAME/ru_RU',\n",
    "                              class_code_column='Hierarchy_MTR_Name'):\n",
    "    \"\"\"\n",
    "    Диагностика проблемы с дообучением\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== ДИАГНОСТИКА ПРОБЛЕМЫ ДООБУЧЕНИЯ ===\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        if new_data_file_path.endswith('.xlsx'):\n",
    "            new_df = pd.read_excel(new_data_file_path)\n",
    "        else:\n",
    "            new_df = pd.read_csv(new_data_file_path, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        # Анализ классов\n",
    "        model_classes = set(label_encoder.classes_)\n",
    "        data_classes = set(new_df[class_code_column].unique())\n",
    "        \n",
    "        common_classes = model_classes.intersection(data_classes)\n",
    "        missing_in_data = model_classes - data_classes\n",
    "        missing_in_model = data_classes - model_classes\n",
    "        \n",
    "        print(f\"Классы в модели: {len(model_classes)}\")\n",
    "        print(f\"Классы в данных: {len(data_classes)}\")\n",
    "        print(f\"Общие классы: {len(common_classes)}\")\n",
    "        print(f\"Классы в модели, но отсутствующие в данных: {len(missing_in_data)}\")\n",
    "        print(f\"Классы в данных, но отсутствующие в модели: {len(missing_in_model)}\")\n",
    "        \n",
    "        # Примеры классов\n",
    "        print(f\"\\nПримеры общих классов: {list(common_classes)[:5]}\")\n",
    "        if missing_in_model:\n",
    "            print(f\"Примеры новых классов: {list(missing_in_model)[:5]}\")\n",
    "        \n",
    "        # Проверка размерности\n",
    "        if model is not None:\n",
    "            try:\n",
    "                output_shape = model.output_shape\n",
    "                print(f\"\\nВыходная shape модели: {output_shape}\")\n",
    "                num_model_classes = output_shape[-1] if isinstance(output_shape, tuple) else output_shape[0][-1]\n",
    "                print(f\"Количество выходов модели: {num_model_classes}\")\n",
    "            except:\n",
    "                print(\"Не удалось определить выходную shape модели\")\n",
    "        \n",
    "        return {\n",
    "            'model_classes_count': len(model_classes),\n",
    "            'data_classes_count': len(data_classes),\n",
    "            'common_classes_count': len(common_classes),\n",
    "            'missing_in_model_count': len(missing_in_model),\n",
    "            'missing_in_data_count': len(missing_in_data)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при диагностике: {e}\")\n",
    "        return None\n",
    "\n",
    "# Запускаем диагностику\n",
    "diagnosis = diagnose_fine_tuning_issue('re_training_tmc_1.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск дообучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример дообучения с объединением наименований\n",
    "if all([model is not None, tokenizer is not None, label_encoder is not None]):\n",
    "    print(\"=== ПРИМЕР ДООБУЧЕНИЯ С ОБЪЕДИНЕНИЕМ НАИМЕНОВАНИЙ ===\")\n",
    "    \n",
    "    success = fine_tune_preserve_weights_fixed_v2(\n",
    "        new_data_file_path='re_training_tmc_1.xlsx',\n",
    "        short_name_column='SHORT_NAME/ru_RU',\n",
    "        full_name_column='FULL_NAME/ru_RU',\n",
    "        class_code_column='Hierarchy_MTR_Name',\n",
    "        class_name_column='Hierarchy_MTR_Class',\n",
    "        epochs=10,\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"✓ Модель успешно дообучена с учетом объединенных наименований!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПРОВЕРКА СОВМЕСТИМОСТИ ===\n",
      "Модель выходов: 1502\n",
      "LabelEncoder классов: 1502\n",
      "✅ Совместимость: OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_model_compatibility():\n",
    "    \"\"\"\n",
    "    Проверяет совместимость модели и label_encoder\n",
    "    \"\"\"\n",
    "    print(\"=== ПРОВЕРКА СОВМЕСТИМОСТИ ===\")\n",
    "    \n",
    "    if model is None or label_encoder is None:\n",
    "        print(\"❌ Модель или LabelEncoder не загружены\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Определяем количество выходов модели\n",
    "        test_input = np.zeros((1, MAX_SEQUENCE_LENGTH), dtype='int32')\n",
    "        test_output = model.predict(test_input, verbose=0)\n",
    "        model_outputs = test_output.shape[1]\n",
    "        \n",
    "        # Количество классов в label_encoder\n",
    "        encoder_classes = len(label_encoder.classes_)\n",
    "        \n",
    "        print(f\"Модель выходов: {model_outputs}\")\n",
    "        print(f\"LabelEncoder классов: {encoder_classes}\")\n",
    "        \n",
    "        if model_outputs == encoder_classes:\n",
    "            print(\"✅ Совместимость: OK\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Несовместимость: {model_outputs} != {encoder_classes}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ошибка проверки: {e}\")\n",
    "        return False\n",
    "\n",
    "# Проверяем перед дообучением\n",
    "check_model_compatibility()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Раздельное дообучение для новых классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_isolated_new_classes(new_data_file_path, \n",
    "                                  short_name_column='SHORT_NAME/ru_RU',\n",
    "                                  full_name_column='FULL_NAME/ru_RU',\n",
    "                                  class_code_column='Hierarchy_MTR_Name', \n",
    "                                  class_name_column='Hierarchy_MTR_Class',\n",
    "                                  epochs_new=10,\n",
    "                                  epochs_fine_tune=5,\n",
    "                                  batch_size=16):\n",
    "    \"\"\"\n",
    "    Изолированное обучение: новые классы обучаются отдельно, затем аккуратно интегрируются\n",
    "    \"\"\"\n",
    "    global model, label_encoder, class_code_to_name\n",
    "    \n",
    "    print(\"=== ИЗОЛИРОВАННОЕ ОБУЧЕНИЕ ДЛЯ НОВЫХ КЛАССОВ ===\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Сохраняем оригинальную модель ДО изменений\n",
    "        print(\"1. Сохранение оригинальной модели...\")\n",
    "        original_model = model\n",
    "        original_weights = model.get_weights()\n",
    "        original_classes = set(label_encoder.classes_)\n",
    "        original_num_classes = len(original_classes)\n",
    "        \n",
    "        # 2. Загружаем новые данные\n",
    "        print(\"2. Загрузка новых данных...\")\n",
    "        if new_data_file_path.endswith('.xlsx'):\n",
    "            new_df = pd.read_excel(new_data_file_path)\n",
    "        else:\n",
    "            new_df = pd.read_csv(new_data_file_path, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        # 3. Анализируем новые классы\n",
    "        new_classes = set(new_df[class_code_column].unique())\n",
    "        new_unique_classes = new_classes - original_classes\n",
    "        \n",
    "        if not new_unique_classes:\n",
    "            print(\"ℹ️ Новых классов не обнаружено\")\n",
    "            return fine_tune_existing_classes_only(new_data_file_path, epochs_fine_tune, batch_size)\n",
    "        \n",
    "        print(f\"⚠ Найдены новые классы: {len(new_unique_classes)}\")\n",
    "        \n",
    "        # 4. Разделяем данные: новые классы и существующие классы\n",
    "        new_classes_df = new_df[new_df[class_code_column].isin(new_unique_classes)]\n",
    "        existing_classes_df = new_df[new_df[class_code_column].isin(original_classes)]\n",
    "        \n",
    "        print(f\"✓ Примеров новых классов: {len(new_classes_df)}\")\n",
    "        print(f\"✓ Примеров существующих классов: {len(existing_classes_df)}\")\n",
    "        \n",
    "        # 5. Обучаем отдельную модель для новых классов\n",
    "        if len(new_classes_df) > 0:\n",
    "            print(\"3. Обучение отдельной модели для новых классов...\")\n",
    "            new_classes_model = train_model_for_new_classes(new_classes_df, epochs_new, batch_size)\n",
    "        else:\n",
    "            new_classes_model = None\n",
    "        \n",
    "        # 6. Аккуратное дообучение основной модели на существующих классах\n",
    "        if len(existing_classes_df) > 0:\n",
    "            print(\"4. Дообучение основной модели на существующих классах...\")\n",
    "            success = fine_tune_on_existing_data(existing_classes_df, epochs_fine_tune, batch_size)\n",
    "            if not success:\n",
    "                print(\"⚠ Ошибка дообучения на существующих классах\")\n",
    "        \n",
    "        # 7. Сохраняем информацию о новых классах для будущего переобучения\n",
    "        save_new_classes_info(new_classes_df, new_unique_classes)\n",
    "        \n",
    "        print(\"✅ Новые классы изолированы и подготовлены для интеграции\")\n",
    "        print(\"💡 Для полной интеграции рекомендуется провести переобучение на всех данных\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка: {e}\")\n",
    "        # Восстанавливаем оригинальную модель в случае ошибки\n",
    "        model = original_model\n",
    "        return False\n",
    "\n",
    "def train_model_for_new_classes(new_classes_df, epochs=10, batch_size=16):\n",
    "    \"\"\"\n",
    "    Обучает отдельную модель только на новых классах\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Объединяем наименования\n",
    "        def combine_names(row):\n",
    "            short_name = str(row['SHORT_NAME/ru_RU']) if pd.notna(row['SHORT_NAME/ru_RU']) else \"\"\n",
    "            full_name = str(row['FULL_NAME/ru_RU']) if pd.notna(row['FULL_NAME/ru_RU']) else \"\"\n",
    "            return f\"{short_name} {full_name}\".strip()\n",
    "        \n",
    "        new_classes_df['combined_text'] = new_classes_df.apply(combine_names, axis=1)\n",
    "        \n",
    "        # Подготовка данных\n",
    "        texts = new_classes_df['combined_text'].apply(preprocess_text).tolist()\n",
    "        labels = new_classes_df['Hierarchy_MTR_Name'].tolist()\n",
    "        \n",
    "        # Создаем отдельный label_encoder для новых классов\n",
    "        new_label_encoder = LabelEncoder()\n",
    "        new_label_encoder.fit(labels)\n",
    "        \n",
    "        # Токенизация\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        X = np.clip(X, 0, vocab_size - 1)\n",
    "        \n",
    "        # Кодирование меток\n",
    "        y = to_categorical(new_label_encoder.transform(labels))\n",
    "        \n",
    "        print(f\"✓ Данные новых классов: X{X.shape}, y{y.shape}\")\n",
    "        print(f\"✓ Количество новых классов: {len(new_label_encoder.classes_)}\")\n",
    "        \n",
    "        # Создаем упрощенную модель для новых классов\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        \n",
    "        new_model = Sequential([\n",
    "            Embedding(vocab_size, 32, input_length=MAX_SEQUENCE_LENGTH),\n",
    "            GlobalAveragePooling1D(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(len(new_label_encoder.classes_), activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        new_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Обучаем модель\n",
    "        history = new_model.fit(\n",
    "            X, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        \n",
    "        # Сохраняем модель новых классов\n",
    "        new_model.save('new_classes_model.h5')\n",
    "        with open('new_classes_label_encoder.pickle', 'wb') as f:\n",
    "            pickle.dump(new_label_encoder, f)\n",
    "        \n",
    "        print(\"✅ Модель для новых классов обучена и сохранена\")\n",
    "        return new_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка обучения модели новых классов: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_new_classes_info(new_classes_df, new_unique_classes):\n",
    "    \"\"\"\n",
    "    Сохраняет информацию о новых классах для будущего переобучения\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Сохраняем данные новых классов\n",
    "        new_classes_df.to_csv('new_classes_data.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "        # Сохраняем mapping новых классов\n",
    "        new_mapping = new_classes_df[['Hierarchy_MTR_Name', 'Hierarchy_MTR_Class']].drop_duplicates()\n",
    "        new_class_mapping = {}\n",
    "        for _, row in new_mapping.iterrows():\n",
    "            new_class_mapping[row['Hierarchy_MTR_Name']] = row['Hierarchy_MTR_Class']\n",
    "        \n",
    "        with open('new_classes_mapping.pickle', 'wb') as f:\n",
    "            pickle.dump(new_class_mapping, f)\n",
    "        \n",
    "        print(f\"✓ Информация о {len(new_unique_classes)} новых классах сохранена\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Ошибка сохранения информации о новых классах: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 1: Восстановление и аккуратное дообучение (рекомендуется)\n",
    "success = fine_tune_isolated_new_classes('re_training_tmc_1.xlsx', epochs_new=10, epochs_fine_tune=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамбль моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def create_ensemble_simple_fixed():\n",
    "    \"\"\"\n",
    "    Создает ансамбль с правильным сохранением моделей в файлы\n",
    "    \"\"\"\n",
    "    global model, label_encoder, tokenizer, preprocess_text\n",
    "    \n",
    "    print(\"=== ПРОСТОЙ АНСАМБЛЬ (С СОХРАНЕНИЕМ В ФАЙЛЫ) ===\")\n",
    "    \n",
    "    try:\n",
    "        # Создаем временную папку для сохранения моделей\n",
    "        temp_dir = 'ensemble_temp'\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "        \n",
    "        # 1. Сохраняем основную модель в файл\n",
    "        main_model_path = os.path.join(temp_dir, 'main_model.h5')\n",
    "        model.save(main_model_path)\n",
    "        print(\"✓ Основная модель сохранена\")\n",
    "        \n",
    "        # 2. Загружаем и сохраняем модель новых классов\n",
    "        from tensorflow.keras.models import load_model\n",
    "        \n",
    "        new_classes_model = load_model('new_classes_model.h5')\n",
    "        new_model_path = os.path.join(temp_dir, 'new_classes_model.h5')\n",
    "        new_classes_model.save(new_model_path)\n",
    "        print(\"✓ Модель новых классов сохранена\")\n",
    "        \n",
    "        # 3. Загружаем остальные компоненты\n",
    "        with open('new_classes_label_encoder.pickle', 'rb') as f:\n",
    "            new_classes_label_encoder = pickle.load(f)\n",
    "        \n",
    "        # 4. Сохраняем все пути к файлам\n",
    "        ensemble_components = {\n",
    "            'main_model_path': main_model_path,\n",
    "            'new_classes_model_path': new_model_path,\n",
    "            'main_label_encoder': label_encoder,\n",
    "            'new_label_encoder': new_classes_label_encoder,\n",
    "            'tokenizer': tokenizer,\n",
    "            'max_seq_length': MAX_SEQUENCE_LENGTH,\n",
    "            'vocab_size': vocab_size,\n",
    "            'temp_dir': temp_dir\n",
    "        }\n",
    "        \n",
    "        with open('ensemble_components.pickle', 'wb') as f:\n",
    "            pickle.dump(ensemble_components, f)\n",
    "        \n",
    "        print(\"✅ Компоненты ансамбля сохранены!\")\n",
    "        return ensemble_components\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def predict_with_ensemble_simple_fixed(texts, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Предсказание с загрузкой моделей из файлов\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('ensemble_components.pickle', 'rb') as f:\n",
    "            components = pickle.load(f)\n",
    "        \n",
    "        # Загружаем модели из файлов\n",
    "        from tensorflow.keras.models import load_model\n",
    "        main_model = load_model(components['main_model_path'])\n",
    "        new_classes_model = load_model(components['new_classes_model_path'])\n",
    "        \n",
    "        print(\"✓ Модели загружены из файлов\")\n",
    "        \n",
    "        # Предобработка текстов\n",
    "        processed_texts = [preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # Токенизация\n",
    "        sequences = components['tokenizer'].texts_to_sequences(processed_texts)\n",
    "        X = pad_sequences(sequences, maxlen=components['max_seq_length'])\n",
    "        X = np.clip(X, 0, components['vocab_size'] - 1)\n",
    "        \n",
    "        # Предсказания основной модели\n",
    "        main_predictions = main_model.predict(X, verbose=0)\n",
    "        main_confidences = np.max(main_predictions, axis=1)\n",
    "        main_classes_idx = np.argmax(main_predictions, axis=1)\n",
    "        main_classes = components['main_label_encoder'].inverse_transform(main_classes_idx)\n",
    "        \n",
    "        # Предсказания модели новых классов\n",
    "        new_predictions = new_classes_model.predict(X, verbose=0)\n",
    "        new_confidences = np.max(new_predictions, axis=1)\n",
    "        new_classes_idx = np.argmax(new_predictions, axis=1)\n",
    "        new_classes = components['new_label_encoder'].inverse_transform(new_classes_idx)\n",
    "        \n",
    "        # Объединяем результаты\n",
    "        results = []\n",
    "        for i in range(len(texts)):\n",
    "            if main_confidences[i] >= confidence_threshold:\n",
    "                final_class = main_classes[i]\n",
    "                final_confidence = main_confidences[i]\n",
    "                source = 'main_model'\n",
    "            elif new_confidences[i] >= confidence_threshold:\n",
    "                final_class = new_classes[i]\n",
    "                final_confidence = new_confidences[i]\n",
    "                source = 'new_classes_model'\n",
    "            else:\n",
    "                if main_confidences[i] >= new_confidences[i]:\n",
    "                    final_class = main_classes[i]\n",
    "                    final_confidence = main_confidences[i]\n",
    "                    source = 'main_model (fallback)'\n",
    "                else:\n",
    "                    final_class = new_classes[i]\n",
    "                    final_confidence = new_confidences[i]\n",
    "                    source = 'new_classes_model (fallback)'\n",
    "            \n",
    "            results.append({\n",
    "                'text': texts[i],\n",
    "                'predicted_class': final_class,\n",
    "                'confidence': final_confidence,\n",
    "                'source_model': source,\n",
    "                'main_model_confidence': main_confidences[i],\n",
    "                'new_model_confidence': new_confidences[i]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка предсказания: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def cleanup_ensemble_files():\n",
    "    \"\"\"\n",
    "    Очищает временные файлы ансамбля\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists('ensemble_components.pickle'):\n",
    "            # Загружаем components чтобы получить путь к temp_dir\n",
    "            with open('ensemble_components.pickle', 'rb') as f:\n",
    "                components = pickle.load(f)\n",
    "            \n",
    "            # Удаляем временную папку\n",
    "            if 'temp_dir' in components and os.path.exists(components['temp_dir']):\n",
    "                shutil.rmtree(components['temp_dir'])\n",
    "                print(f\"✓ Удалена временная папка: {components['temp_dir']}\")\n",
    "            \n",
    "            # Удаляем файл components\n",
    "            os.remove('ensemble_components.pickle')\n",
    "            print(\"✓ Удален файл ensemble_components.pickle\")\n",
    "        \n",
    "        print(\"✅ Временные файлы ансамбля очищены\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Ошибка очистки: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 2: Простой функциональный ансамбль\n",
    "print(\"\\n=== ВАРИАНТ 2: ПРОСТОЙ АНСАМБЛЬ ===\")\n",
    "components = create_ensemble_simple_fixed()\n",
    "    # Тестируем ансамбль\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "        \"Отвод П90-57х4,5-12Х18Н10Т ГОСТ 17375-2001\",  # Старый класс\n",
    "        \"Отвод стальной 90 градусов\",    # Новый класс\n",
    "        \"Отвод крутоиз.90гр-89х6 13ХФА Г17375\"     # Старый класс\n",
    "    ]\n",
    "if components:\n",
    "    results = predict_with_ensemble_simple_fixed(test_texts)\n",
    "    for result in results:\n",
    "        print(f\"Текст: {result['text']}\")\n",
    "        print(f\"Класс: {result['predicted_class']}\")\n",
    "        print(f\"Уверенность: {result['confidence']:.3f}\")\n",
    "        print(f\"Модель: {result['source_model']}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамбль с ECXEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_excel_with_ensemble(input_file_path, output_file_path, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Классификация Excel файла с использованием ансамбля\n",
    "    \"\"\"\n",
    "    print(\"=== КЛАССИФИКАЦИЯ EXCEL С АНСАМБЛЕМ ===\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем компоненты ансамбля\n",
    "        with open('ensemble_components.pickle', 'rb') as f:\n",
    "            components = pickle.load(f)\n",
    "        \n",
    "        # Загружаем данные для классификации\n",
    "        df_input = pd.read_excel(input_file_path)\n",
    "        \n",
    "        # Автопоиск столбца с текстом\n",
    "        text_columns = []\n",
    "        for col in df_input.columns:\n",
    "            col_lower = col.lower()\n",
    "            if any(keyword in col_lower for keyword in ['наименование', 'name', 'описание', 'description', 'текст', 'text']):\n",
    "                text_columns.append(col)\n",
    "        \n",
    "        text_column = text_columns[0] if text_columns else df_input.columns[0]\n",
    "        print(f\"✓ Используется столбец: '{text_column}'\")\n",
    "        \n",
    "        # Классификация\n",
    "        results = []\n",
    "        for idx, row in df_input.iterrows():\n",
    "            text = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "            \n",
    "            if not text.strip():\n",
    "                result_row = {\n",
    "                    'Исходное_наименование': text,\n",
    "                    'Код_класса': None,\n",
    "                    'Наименование_класса': 'НЕ ОПРЕДЕЛЕНО',\n",
    "                    'Уверенность': 0.0,\n",
    "                    'Статус_уверенности': 'ПУСТАЯ_СТРОКА',\n",
    "                    'Модель_источник': 'N/A'\n",
    "                }\n",
    "            else:\n",
    "                try:\n",
    "                    # Предсказание с ансамблем\n",
    "                    prediction_result = predict_with_ensemble_simple([text], confidence_threshold)[0]\n",
    "                    \n",
    "                    # Получаем название класса из mapping\n",
    "                    class_name = class_code_to_name.get(prediction_result['predicted_class'], \n",
    "                                                       f\"Class_{prediction_result['predicted_class']}\")\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': prediction_result['predicted_class'],\n",
    "                        'Наименование_класса': class_name,\n",
    "                        'Уверенность': prediction_result['confidence'],\n",
    "                        'Статус_уверенности': 'ВЫСОКАЯ' if prediction_result['confidence'] > 0.7 else \n",
    "                                            'СРЕДНЯЯ' if prediction_result['confidence'] > 0.5 else 'НИЗКАЯ',\n",
    "                        'Модель_источник': prediction_result['source_model']\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    result_row = {\n",
    "                        'Исходное_наименование': text,\n",
    "                        'Код_класса': None,\n",
    "                        'Наименование_класса': 'ОШИБКА',\n",
    "                        'Уверенность': 0.0,\n",
    "                        'Статус_уверенности': f'ОШИБКА: {str(e)}',\n",
    "                        'Модель_источник': 'N/A'\n",
    "                    }\n",
    "            \n",
    "            # Добавляем исходные колонки\n",
    "            for col in df_input.columns:\n",
    "                result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Обработано {idx + 1} строк...\")\n",
    "        \n",
    "        # Сохраняем результаты\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.to_excel(output_file_path, index=False)\n",
    "        \n",
    "        # Статистика\n",
    "        total = len(df_results)\n",
    "        high_confidence = len([r for r in results if r['Статус_уверенности'] == 'ВЫСОКАЯ'])\n",
    "        main_model_used = len([r for r in results if 'main_model' in r.get('Модель_источник', '')])\n",
    "        new_model_used = len([r for r in results if 'new_classes_model' in r.get('Модель_источник', '')])\n",
    "        \n",
    "        print(f\"✅ Результаты сохранены в: {output_file_path}\")\n",
    "        print(f\"📊 Статистика:\")\n",
    "        print(f\"   Всего строк: {total}\")\n",
    "        print(f\"   Высокая уверенность: {high_confidence} ({high_confidence/total*100:.1f}%)\")\n",
    "        print(f\"   Основная модель: {main_model_used} строк\")\n",
    "        print(f\"   Модель новых классов: {new_model_used} строк\")\n",
    "        \n",
    "        return df_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ошибка: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_excel_with_ensemble('input.xlsx', 'output_ensemble.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
